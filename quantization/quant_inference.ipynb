{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/scratch/joluseti/local_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with the MNIST mean and std\n",
    "])\n",
    "\n",
    "\n",
    "train_data = datasets.MNIST(root=data_path, train=True, download=False, transform=transform)\n",
    "test_data = datasets.MNIST(root=data_path, train=False, download=False, transform=transform)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLenet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = 5, bias=False)\n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2) \n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = 5, bias=False)\n",
    "        self.fc1 = nn.Linear(in_features = 16 * 4 * 4, out_features = 120, bias = False)\n",
    "        self.fc2 = nn.Linear(in_features = 120, out_features = 84, bias=False)\n",
    "        self.fc3 = nn.Linear(in_features = 84, out_features = 10, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        y = x.reshape(-1, 16 * 4 * 4)\n",
    "        x = x.reshape(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.dequant(x)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_file):\n",
    "    #model = QuantLenet()\n",
    "    model = torch.load(model_file, weights_only=False)\n",
    "    #model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    device = \"cpu\"        \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def calibrate(model, data_loader, cal_batch_sz = 10):\n",
    "    model.eval()\n",
    "    print(\"Calibration Start...\")\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:  # Pass some sample data\n",
    "            model(data)\n",
    "            cal_batch_sz -= 1\n",
    "            if cal_batch_sz == 0:\n",
    "                break\n",
    "    print(\"Calibration Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/scratch/joluseti/618/model/float_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.serialization.add_safe_globals([QuantLenet()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLenet(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=False)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=False)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=False)\n",
       "  (relu1): ReLU()\n",
       "  (relu2): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_model = load_model(model_path).to('cpu')\n",
    "float_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['conv1', 'relu1'], ['conv2', 'relu2']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules_to_fuse = [[f'conv{i}', f'relu{i}'] for i in range(1, 3)]\n",
    "modules_to_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLenet(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): ConvReLU2d(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=False)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=False)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=False)\n",
       "  (relu1): Identity()\n",
       "  (relu2): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model = torch.ao.quantization.fuse_modules(float_model, modules_to_fuse)\n",
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.178993\n",
      "Test Accuracy: 98.54%\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(float_model)\n",
    "print(f\"Test Accuracy: {evaluate(float_model, test_loader):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.179057\n",
      "Test Accuracy: 98.54%\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(quant_model)\n",
    "print(f\"Test Accuracy: {evaluate(quant_model, test_loader):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration Start...\n",
      "Calibration Done...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantLenet(\n",
       "  (quant): Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027164967730641365, zero_point=0)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04062113165855408, zero_point=0)\n",
       "  (fc1): QuantizedLinear(in_features=256, out_features=120, scale=0.1486314982175827, zero_point=135, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=0.12291006743907928, zero_point=114, qscheme=torch.per_tensor_affine)\n",
       "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.17384813725948334, zero_point=142, qscheme=torch.per_tensor_affine)\n",
       "  (relu1): Identity()\n",
       "  (relu2): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "quant_model.qconfig = torch.ao.quantization.get_default_qconfig('qnnpack')\n",
    "torch.ao.quantization.prepare(quant_model, inplace=True)\n",
    "calibrate(quant_model, train_loader, cal_batch_sz = 40)\n",
    "torch.ao.quantization.convert(quant_model, inplace=True)\n",
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 0.05191\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(quant_model)\n",
    "#print(f\"Test Accuracy: {evaluate(quant_model, test_loader):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(float_model), '/scratch/joluseti/618/test/float.pth')\n",
    "torch.jit.save(torch.jit.script(quant_model), '/scratch/joluseti/618/test/quant.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ao.quantization.get_default_qconfig(\"qnnpack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    }
   ],
   "source": [
    "print(torch.ao.quantization.get_default_qconfig(\"x86\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Quantized model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('/scratch/joluseti/618/test/quant.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3729, -0.8581,  0.3432,  3.0890, -2.5742,  0.1716, -3.6039,  0.0000,\n",
       "           1.2013, -0.3432]]),\n",
       " tensor([[0.3714, 0.0000, 1.0728, 0.1651, 0.3714, 0.0000, 0.6602, 0.4952, 0.0825,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0316, 0.0000, 0.2476, 0.2888,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0413, 0.0413, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.6190, 1.1554, 0.0000, 0.4126, 0.7015, 0.3301, 0.0000,\n",
       "          0.0825, 0.4952, 0.7840, 1.3204, 0.0000, 0.0000, 0.0000, 0.3301, 0.2888,\n",
       "          0.7840, 1.0316, 1.1554, 0.2888, 0.6190, 0.8665, 0.1238, 0.7840, 0.0000,\n",
       "          0.3301, 0.1238, 0.0000, 0.2063, 0.9078, 0.7015, 0.7015, 0.4539, 0.0000,\n",
       "          0.0000, 1.7331, 1.6918, 1.4030, 2.3520, 0.9078, 2.0632, 2.1044, 2.3520,\n",
       "          2.6821, 2.2695, 1.8156, 1.4030, 2.1870, 2.3933, 3.3011, 3.6312, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1554, 2.2282, 1.9394, 0.8665,\n",
       "          0.0000, 0.3301, 0.8253, 0.8665, 0.0000, 0.0000, 0.0000, 1.1141, 0.0000,\n",
       "          0.9078, 0.0825, 0.5364, 0.0000, 0.7840, 0.0000, 0.0000, 0.0000, 0.9903,\n",
       "          0.0000, 0.0000, 1.3617, 1.0316, 0.7015, 0.9903, 1.5267, 1.6093, 0.7015,\n",
       "          0.5364, 1.1554, 1.3617, 1.4855, 1.4442, 0.2888, 0.1651, 0.6602, 1.1554,\n",
       "          1.6505, 1.3204, 1.4030, 1.1554, 1.1554, 1.2792, 1.1554, 1.6918, 1.2379,\n",
       "          0.7840, 1.0728, 1.7743, 1.7331, 1.5680, 1.9394, 1.2792, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5364, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.8665, 0.0413, 1.6093, 2.2695, 2.1044, 1.2792,\n",
       "          1.8981, 1.7743, 1.8156, 2.0219, 1.0316, 1.2379, 1.4855, 2.1870, 1.8156,\n",
       "          1.7743, 2.3108, 1.6505, 0.9078, 0.6602, 1.0316, 0.7015, 0.6602, 0.7840,\n",
       "          0.6602, 0.8665, 0.7015, 0.5777, 0.3301, 1.1141, 0.3714, 0.3714, 1.2379,\n",
       "          0.3301, 0.4539, 0.0000, 0.5777, 0.4539, 0.7015, 0.5777, 0.1238, 0.2888,\n",
       "          0.7427, 1.0728, 0.4539, 0.6602, 0.2476, 0.0000, 0.6602, 1.2792, 0.0000,\n",
       "          0.6602, 0.0000, 1.0316, 0.7427, 0.6602, 0.3714, 1.4442, 0.0000, 0.3714,\n",
       "          0.4539, 0.0000, 0.0000, 1.5680, 1.1554, 0.8665, 1.7331, 1.1141, 0.9078,\n",
       "          0.4539, 0.0000, 0.6602, 0.4539, 0.7427, 0.8665, 0.6190, 0.9078, 1.6918,\n",
       "          0.6602, 0.4539, 0.0000, 0.0000]], size=(1, 256), dtype=torch.quint8,\n",
       "        quantization_scheme=torch.per_tensor_affine, scale=0.0412634015083313,\n",
       "        zero_point=0))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((1,1,28,28))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to see output in different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "view_model = copy.deepcopy(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of layer Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8):torch.Size([1, 1, 28, 28])\n",
      "Output of layer QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027150312438607216, zero_point=0):torch.Size([1, 6, 24, 24])\n",
      "Output of layer Identity():torch.Size([1, 6, 24, 24])\n",
      "Output of layer MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False):torch.Size([1, 16, 4, 4])\n",
      "Output of layer QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04114341735839844, zero_point=0):torch.Size([1, 16, 8, 8])\n",
      "Output of layer Identity():torch.Size([1, 16, 8, 8])\n",
      "Output of layer QuantizedLinear(in_features=256, out_features=120, scale=0.1537804752588272, zero_point=137, qscheme=torch.per_tensor_affine):torch.Size([1, 120])\n",
      "Output of layer QuantizedLinear(in_features=120, out_features=84, scale=0.12323486804962158, zero_point=116, qscheme=torch.per_tensor_affine):torch.Size([1, 84])\n",
      "Output of layer QuantizedLinear(in_features=84, out_features=10, scale=0.17416171729564667, zero_point=142, qscheme=torch.per_tensor_affine):torch.Size([1, 10])\n",
      "Output of layer DeQuantize():torch.Size([1, 10])\n",
      "Output of layer QuantLenet(\n",
      "  (quant): Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)\n",
      "  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027150312438607216, zero_point=0)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04114341735839844, zero_point=0)\n",
      "  (fc1): QuantizedLinear(in_features=256, out_features=120, scale=0.1537804752588272, zero_point=137, qscheme=torch.per_tensor_affine)\n",
      "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=0.12323486804962158, zero_point=116, qscheme=torch.per_tensor_affine)\n",
      "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.17416171729564667, zero_point=142, qscheme=torch.per_tensor_affine)\n",
      "  (relu1): Identity()\n",
      "  (relu2): Identity()\n",
      "  (dequant): DeQuantize()\n",
      "):torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'quant_model' is your loaded quantized model\n",
    "quant_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Dictionary to store outputs\n",
    "layer_outputs = {}\n",
    "\n",
    "# Define a hook function to capture outputs\n",
    "def hook_fn(module, input, output):\n",
    "    # Store the output of the layer\n",
    "    layer_outputs[module] = output\n",
    "\n",
    "# Attach the hook to each layer in the model\n",
    "for name, module in view_model.named_modules():\n",
    "    module.register_forward_hook(hook_fn)\n",
    "\n",
    "# Example input (replace with the actual input for your model)\n",
    "input_tensor = torch.randn(1, 1, 28, 28)  # Example: batch size 1, 1 channel, 28x28 image\n",
    "input_tensor = input_tensor.to(torch.float32)  # Ensure input is float32 for quantization\n",
    "\n",
    "# Perform a forward pass through the model\n",
    "with torch.no_grad():\n",
    "    view_model(input_tensor)\n",
    "\n",
    "# Print the outputs for each layer\n",
    "for layer, output in layer_outputs.items():\n",
    "    print(f\"Output of layer {layer}:{output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)\n",
      "QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027150312438607216, zero_point=0)\n",
      "Identity()\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04114341735839844, zero_point=0)\n",
      "Identity()\n",
      "QuantizedLinear(in_features=256, out_features=120, scale=0.1537804752588272, zero_point=137, qscheme=torch.per_tensor_affine)\n",
      "QuantizedLinear(in_features=120, out_features=84, scale=0.12323486804962158, zero_point=116, qscheme=torch.per_tensor_affine)\n",
      "QuantizedLinear(in_features=84, out_features=10, scale=0.17416171729564667, zero_point=142, qscheme=torch.per_tensor_affine)\n",
      "DeQuantize()\n",
      "QuantLenet(\n",
      "  (quant): Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)\n",
      "  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027150312438607216, zero_point=0)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04114341735839844, zero_point=0)\n",
      "  (fc1): QuantizedLinear(in_features=256, out_features=120, scale=0.1537804752588272, zero_point=137, qscheme=torch.per_tensor_affine)\n",
      "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=0.12323486804962158, zero_point=116, qscheme=torch.per_tensor_affine)\n",
      "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.17416171729564667, zero_point=142, qscheme=torch.per_tensor_affine)\n",
      "  (relu1): Identity()\n",
      "  (relu2): Identity()\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for m in layer_outputs.keys():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  32,   41,   -5,  -45,   60],\n",
       "          [ -28,   57,  -10,    1,   68],\n",
       "          [  88,   17,  -74,  -68,   61],\n",
       "          [  31,   41,  -57,  -98,  -34],\n",
       "          [  10,   36,   51,    4,   40]]],\n",
       "\n",
       "\n",
       "        [[[ -49,  -27,  -16,   27,   19],\n",
       "          [ -87,  -16,   86,  -15,   -5],\n",
       "          [ -57,  -78,   80,   51,    0],\n",
       "          [ -47,  -56,   92,   17,  -26],\n",
       "          [-114,   -1,   41,   75,  -16]]],\n",
       "\n",
       "\n",
       "        [[[ -14,   50,  -28,   46,  -21],\n",
       "          [ -46,   40,   73,   41,   56],\n",
       "          [  64,   73,  -45,  -18,   24],\n",
       "          [ -51,   11,   51,   93,   43],\n",
       "          [  51,   44,  -58,   43,   79]]],\n",
       "\n",
       "\n",
       "        [[[  48,   40,   97,   80,   76],\n",
       "          [  63,  -16,  -29,   48,   66],\n",
       "          [ -15,   48,  -92,  -26,   54],\n",
       "          [ -38,  -73,  -18,  -99,  -23],\n",
       "          [ -84,  -25,    9,  -64,  -42]]],\n",
       "\n",
       "\n",
       "        [[[ -41,   87,   38,   -5,   12],\n",
       "          [   6,   34,   76,   -8,  -77],\n",
       "          [  11,   19,   80,   55,  -34],\n",
       "          [  -8,   73,   34,  -18,  -62],\n",
       "          [  62,   64,    5,  -21,   -3]]],\n",
       "\n",
       "\n",
       "        [[[  11,  -85, -108, -102,  -26],\n",
       "          [  35,   23,   24, -114, -128],\n",
       "          [  86,   18,   26,  -14,  -61],\n",
       "          [  55,   25,  103,   55,  111],\n",
       "          [ -49,   56,   64,   85,   62]]]], dtype=torch.int8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.conv1.weight().int_repr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 1\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print(f'Elapsed time: {elapsed/num_images*1000} ms' )\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0.03268010914325714 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008366107940673828"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_benchmark('/scratch/joluseti/618/test/float.pth', test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 8.926535956561565 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.2851932048797607"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_benchmark('/scratch/joluseti/618/test/quant.pth', test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'conv1', 'conv2', 'cpu', 'cuda', 'dequant', 'double', 'dump_patches', 'eval', 'extra_repr', 'fc1', 'fc2', 'fc3', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'pool', 'quant', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'relu1', 'relu2', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "quant ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dtype', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'scale', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad', 'zero_point']\n",
      "conv1 ['T_destination', '_FLOAT_MODULE', '_NNIQAT_CONV_BN_MODULE', '_NNI_CONV_ADD_MODULE', '_NNI_CONV_ADD_RELU_MODULE', '_NNI_CONV_RELU_MODULE', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_init', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'dilation', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'from_reference', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_qconv', 'get_submodule', 'groups', 'half', 'in_channels', 'ipu', 'kernel_size', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_channels', 'output_padding', 'padding', 'padding_mode', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'scale', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'stride', 'to', 'to_empty', 'train', 'training', 'transposed', 'type', 'weight', 'xpu', 'zero_grad', 'zero_point']\n",
      "pool ['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'ceil_mode', 'children', 'compile', 'cpu', 'cuda', 'dilation', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'kernel_size', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'padding', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'return_indices', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'stride', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "conv2 ['T_destination', '_FLOAT_MODULE', '_NNIQAT_CONV_BN_MODULE', '_NNI_CONV_ADD_MODULE', '_NNI_CONV_ADD_RELU_MODULE', '_NNI_CONV_RELU_MODULE', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_init', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'dilation', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'from_reference', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_qconv', 'get_submodule', 'groups', 'half', 'in_channels', 'ipu', 'kernel_size', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_channels', 'output_padding', 'padding', 'padding_mode', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'scale', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'stride', 'to', 'to_empty', 'train', 'training', 'transposed', 'type', 'weight', 'xpu', 'zero_grad', 'zero_point']\n",
      "fc1 ['T_destination', '_FLOAT_MODULE', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'from_reference', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in_features', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'scale', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad', 'zero_point']\n",
      "fc1._packed_params ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dtype', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "fc2 ['T_destination', '_FLOAT_MODULE', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'from_reference', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in_features', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'scale', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad', 'zero_point']\n",
      "fc2._packed_params ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dtype', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "fc3 ['T_destination', '_FLOAT_MODULE', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'from_reference', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in_features', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'scale', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad', 'zero_point']\n",
      "fc3._packed_params ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_packed_params', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_weight_bias', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dtype', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'set_weight_bias', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "relu1 ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "relu2 ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n",
      "dequant ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__overloads__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'from_float', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "for name, module in quant_model.named_modules():\n",
    "    print(name, dir(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 'quant' has a weight method.\n",
      "tensor([33])\n",
      "Module 'conv1' has a weight method.\n",
      "0\n",
      "Module 'conv2' has a weight method.\n",
      "0\n",
      "Module 'fc1' has a weight method.\n",
      "136\n",
      "Module 'fc2' has a weight method.\n",
      "115\n",
      "Module 'fc3' has a weight method.\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "for name, module in quant_model.named_modules():\n",
    "    if hasattr(module, 'scale'):\n",
    "        print(f\"Module '{name}' has a weight method.\")\n",
    "        #print(module.weight().int_repr().min())\n",
    "        print(module.zero_point)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module 'quant' has a weight method.\n",
      "tensor([0.0127])\n",
      "Module 'conv1' has a weight method.\n",
      "0.027446914464235306\n",
      "Module 'conv2' has a weight method.\n",
      "0.04141329973936081\n",
      "Module 'fc1' has a weight method.\n",
      "0.1510031670331955\n",
      "Module 'fc2' has a weight method.\n",
      "0.12524110078811646\n",
      "Module 'fc3' has a weight method.\n",
      "0.17506888508796692\n"
     ]
    }
   ],
   "source": [
    "for name, module in quant_model.named_modules():\n",
    "    if hasattr(module, 'scale'):\n",
    "        print(f\"Module '{name}' has a weight method.\")\n",
    "        #print(module.weight().int_repr().min())\n",
    "        print(module.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0962,  0.0481, -0.0272, -0.0251, -0.0502,  0.0857,  0.0000, -0.0188,\n",
       "          0.0314, -0.1004,  0.1087, -0.0941,  0.1338,  0.0606,  0.0586,  0.0314,\n",
       "          0.1192, -0.0167, -0.1066, -0.1066,  0.0125, -0.1004, -0.0293, -0.0816,\n",
       "          0.0962,  0.1108, -0.1715, -0.0565,  0.0125, -0.1046, -0.0167, -0.0941,\n",
       "         -0.0167, -0.0774, -0.0376,  0.0084,  0.0272, -0.0753, -0.0523, -0.1025,\n",
       "          0.0648, -0.0481, -0.0084, -0.1087,  0.0565,  0.0836, -0.0146,  0.0669,\n",
       "         -0.1297,  0.0857, -0.0251, -0.0293, -0.0376, -0.0606, -0.1777, -0.0230,\n",
       "          0.0146, -0.1798, -0.0293,  0.0816, -0.0439, -0.0167, -0.1861, -0.1066,\n",
       "          0.0230,  0.1046, -0.0209, -0.0669, -0.0836, -0.0983,  0.0335, -0.0125,\n",
       "         -0.0397, -0.0983,  0.1422,  0.0544,  0.0000,  0.1380,  0.0314, -0.0795,\n",
       "          0.0084, -0.0690,  0.0188, -0.0293],\n",
       "        [ 0.1422,  0.0753, -0.0355, -0.0021,  0.0795,  0.0460, -0.0230,  0.0209,\n",
       "          0.0272,  0.0920,  0.0669, -0.0376,  0.0314, -0.1840, -0.0084, -0.0125,\n",
       "         -0.1317,  0.0836, -0.0439, -0.0502, -0.1757,  0.0293,  0.0376, -0.0021,\n",
       "         -0.0690, -0.0084,  0.0962,  0.0105, -0.0188,  0.0502, -0.0293, -0.0418,\n",
       "          0.0544, -0.0314,  0.1004, -0.1464,  0.0941,  0.0669,  0.0523,  0.1087,\n",
       "         -0.1213, -0.0146,  0.0565,  0.0063, -0.0899,  0.1129, -0.1422,  0.0962,\n",
       "          0.0753, -0.1506,  0.1297,  0.1422, -0.2112, -0.0899, -0.1589, -0.1631,\n",
       "         -0.0272, -0.0669, -0.0753, -0.0690,  0.0042,  0.1380, -0.0816, -0.0753,\n",
       "         -0.0878,  0.0586,  0.0084,  0.1380,  0.0000,  0.1568, -0.0753, -0.0146,\n",
       "          0.0753, -0.1589,  0.1046, -0.0565, -0.0836,  0.0544, -0.0376,  0.0544,\n",
       "          0.0795,  0.0753, -0.1652, -0.1443],\n",
       "        [ 0.0439,  0.0000,  0.0251,  0.1485,  0.1192,  0.1066, -0.1087, -0.0983,\n",
       "         -0.0105, -0.1025, -0.1046, -0.0314, -0.0397, -0.0962,  0.1066, -0.0899,\n",
       "          0.0816, -0.0314, -0.1443,  0.0669, -0.0125, -0.1046,  0.0397, -0.0146,\n",
       "         -0.0816,  0.1359,  0.0251, -0.0962,  0.0063,  0.0439,  0.0816, -0.0042,\n",
       "          0.1150,  0.0669, -0.1736,  0.0899, -0.1798,  0.0314,  0.0146, -0.1297,\n",
       "          0.0606, -0.0899, -0.0586, -0.0251, -0.0774, -0.0669, -0.0920,  0.0021,\n",
       "         -0.0418,  0.0690,  0.0878, -0.0606, -0.1129,  0.0648, -0.1213, -0.0335,\n",
       "          0.0878, -0.1777,  0.0606,  0.0418, -0.0293,  0.0146, -0.1422,  0.1025,\n",
       "         -0.0962, -0.0397,  0.0816,  0.0146, -0.1213,  0.0544, -0.0899, -0.0669,\n",
       "         -0.0021,  0.0627, -0.0125,  0.0920,  0.0418, -0.1108,  0.0544, -0.0711,\n",
       "          0.0460,  0.0000, -0.0878,  0.1276],\n",
       "        [ 0.0105, -0.1443,  0.0732, -0.0648,  0.1527, -0.0418,  0.0355, -0.0669,\n",
       "         -0.1025, -0.0753, -0.0606,  0.0836, -0.1380, -0.1046, -0.0586,  0.0878,\n",
       "          0.1046, -0.0125, -0.1276,  0.0586, -0.1171,  0.0711, -0.1004, -0.0251,\n",
       "          0.1276, -0.1046,  0.0418, -0.0523, -0.1485,  0.1046, -0.0502,  0.0272,\n",
       "         -0.1757,  0.0209,  0.1192,  0.0251, -0.1066, -0.1025, -0.0376, -0.0795,\n",
       "          0.0523, -0.0042, -0.0251,  0.0188,  0.1129,  0.1443, -0.0272,  0.0857,\n",
       "          0.0606,  0.1276,  0.0188, -0.1297,  0.0669, -0.0209, -0.0899,  0.0606,\n",
       "         -0.0167,  0.0816, -0.1066,  0.0335, -0.0063,  0.0063,  0.1108, -0.0732,\n",
       "         -0.0125, -0.0314, -0.0711,  0.1046, -0.0021, -0.2279, -0.0878,  0.0711,\n",
       "         -0.0857,  0.0586,  0.1025, -0.0188,  0.0836, -0.0502, -0.1443,  0.0899,\n",
       "          0.1046,  0.0774,  0.1359, -0.1422],\n",
       "        [-0.0063,  0.0878,  0.0753,  0.0167,  0.0125,  0.0230, -0.0355,  0.0565,\n",
       "         -0.1568,  0.0146, -0.0042, -0.0418,  0.0857,  0.0899, -0.1464,  0.0418,\n",
       "         -0.0899, -0.0251,  0.0648,  0.1108, -0.1234,  0.0983, -0.0230,  0.0920,\n",
       "         -0.1631, -0.1631,  0.0606, -0.0125,  0.1317,  0.0084,  0.0042, -0.0188,\n",
       "          0.0460,  0.0293, -0.1108, -0.1213,  0.0355, -0.1234, -0.0125, -0.0983,\n",
       "         -0.0878, -0.0272,  0.0481,  0.0105,  0.0899, -0.1485,  0.0335, -0.0502,\n",
       "          0.0105,  0.0544,  0.0878,  0.1297, -0.0021, -0.0544,  0.0439,  0.0586,\n",
       "         -0.1046, -0.0544, -0.0648, -0.0209, -0.0774,  0.0125,  0.1317, -0.0209,\n",
       "          0.0544, -0.0125,  0.1150,  0.0920, -0.0146,  0.0899,  0.1066,  0.0899,\n",
       "         -0.0314,  0.0105, -0.1694, -0.0920,  0.1297,  0.0795,  0.0335, -0.1485,\n",
       "         -0.0565, -0.0899,  0.0690, -0.0962],\n",
       "        [-0.0795,  0.0669, -0.0816, -0.0063, -0.0774, -0.0857,  0.1317,  0.0753,\n",
       "         -0.0439, -0.0021,  0.1443, -0.0042, -0.1192,  0.0146,  0.1129,  0.0544,\n",
       "          0.0230, -0.1757, -0.1443,  0.0544,  0.0042, -0.0335,  0.0753, -0.0021,\n",
       "         -0.0816,  0.0523,  0.1317, -0.0230, -0.1171, -0.1004,  0.0167, -0.0230,\n",
       "         -0.0355, -0.0481,  0.0648, -0.0209, -0.1506,  0.0251, -0.0690,  0.1527,\n",
       "         -0.0523, -0.0732, -0.1777,  0.0816,  0.0690,  0.0502,  0.1380, -0.0669,\n",
       "          0.0397, -0.0335,  0.0084, -0.1694,  0.0209,  0.0732,  0.0105, -0.1401,\n",
       "         -0.1317,  0.0606,  0.0167,  0.0293, -0.0753, -0.0084,  0.0586, -0.0272,\n",
       "          0.0293,  0.0836, -0.0627, -0.1736, -0.0125,  0.1066,  0.1276, -0.0209,\n",
       "         -0.0586, -0.1046, -0.0523, -0.0857,  0.0732,  0.1150, -0.0167, -0.0314,\n",
       "          0.0878,  0.0355, -0.0042,  0.1087],\n",
       "        [-0.1861,  0.1087, -0.0920, -0.0669,  0.0627,  0.0962, -0.0669,  0.0272,\n",
       "         -0.0146, -0.0816,  0.0983,  0.0586,  0.0000, -0.0690,  0.0669, -0.0878,\n",
       "         -0.1443, -0.0397,  0.0397, -0.0021,  0.0732, -0.1443,  0.0586, -0.0648,\n",
       "          0.1108,  0.1234, -0.1255, -0.0816,  0.1317, -0.0648,  0.0042, -0.0690,\n",
       "         -0.0460,  0.0753, -0.0732, -0.0418, -0.0941, -0.1255, -0.0209,  0.1129,\n",
       "          0.0774, -0.0335,  0.0627,  0.0397, -0.0251,  0.0627,  0.0774, -0.0460,\n",
       "         -0.1443, -0.0293, -0.1819, -0.0941, -0.0648,  0.1066,  0.0899, -0.1903,\n",
       "         -0.1610, -0.0962,  0.0523,  0.1234, -0.2049, -0.1171, -0.1359, -0.0439,\n",
       "          0.0669,  0.1234,  0.0105, -0.0481,  0.0586,  0.0836, -0.0042,  0.1066,\n",
       "         -0.0481, -0.0167, -0.0941, -0.0816,  0.1129,  0.0565, -0.1819,  0.0209,\n",
       "         -0.1443, -0.1861,  0.1108, -0.0021],\n",
       "        [-0.1046,  0.1631, -0.0941, -0.0439, -0.0397, -0.1757,  0.0460,  0.0209,\n",
       "          0.1757,  0.0648,  0.1338, -0.0669, -0.0711, -0.1087, -0.1171, -0.0648,\n",
       "          0.0167,  0.1443,  0.0042, -0.1192, -0.0899,  0.1129, -0.0795, -0.0021,\n",
       "          0.0523, -0.0795,  0.0209, -0.0732, -0.0544,  0.0021,  0.0836,  0.0335,\n",
       "          0.1192,  0.0795, -0.1757, -0.0732,  0.1046, -0.1589, -0.0460, -0.1276,\n",
       "          0.1046, -0.0439, -0.0690,  0.0397,  0.0774, -0.0439, -0.0983,  0.0544,\n",
       "          0.0021, -0.1066,  0.1046, -0.0586,  0.0544, -0.0230,  0.0669, -0.0627,\n",
       "         -0.0063,  0.0732, -0.1046,  0.0230,  0.1297,  0.1631, -0.0230, -0.0293,\n",
       "          0.0753,  0.0732,  0.0439,  0.0063, -0.1715, -0.1276, -0.1297,  0.0084,\n",
       "         -0.1171,  0.1359, -0.0586, -0.0042, -0.0293,  0.1255,  0.0439,  0.0627,\n",
       "          0.0753,  0.1506, -0.0293,  0.0272],\n",
       "        [ 0.0397,  0.0376, -0.0105, -0.0251,  0.0606, -0.1568,  0.1108,  0.0418,\n",
       "         -0.0439, -0.1066, -0.2677,  0.1004, -0.0272, -0.1359,  0.0439,  0.0042,\n",
       "          0.0544,  0.1171, -0.0230, -0.0983,  0.0690, -0.1380,  0.0376, -0.0125,\n",
       "         -0.1150,  0.0063,  0.0439,  0.0941,  0.0836,  0.0523, -0.0355,  0.0335,\n",
       "         -0.1359, -0.0962,  0.0272, -0.1192, -0.1568, -0.0439, -0.1171,  0.0293,\n",
       "         -0.0795,  0.0397,  0.0335,  0.0105, -0.0293, -0.0544, -0.0544,  0.0857,\n",
       "         -0.1025,  0.0774, -0.0251, -0.1736,  0.0711,  0.1255,  0.0816,  0.0962,\n",
       "          0.1527,  0.0836, -0.0627, -0.0105, -0.0983, -0.1464, -0.0397, -0.0648,\n",
       "          0.1150,  0.1046, -0.0753, -0.0502,  0.1485, -0.1401,  0.1297,  0.0920,\n",
       "          0.0544,  0.1213, -0.0251,  0.0481, -0.0335, -0.0146,  0.0146, -0.0920,\n",
       "          0.1297, -0.1736, -0.0606, -0.1485],\n",
       "        [ 0.0000,  0.0481, -0.1046,  0.0063,  0.0983, -0.0042, -0.0795,  0.0209,\n",
       "         -0.0753, -0.0544, -0.0669,  0.0188, -0.0565,  0.0816,  0.1087, -0.0021,\n",
       "          0.0125, -0.0209, -0.0606, -0.0920,  0.0690,  0.0544, -0.0397, -0.0084,\n",
       "         -0.1506, -0.0188,  0.0502, -0.0648,  0.0795, -0.1694,  0.0857, -0.0251,\n",
       "         -0.1506, -0.2154, -0.0084,  0.0544,  0.0627, -0.0293,  0.0439, -0.0042,\n",
       "         -0.0335, -0.0920, -0.0795,  0.0690,  0.0669, -0.1046,  0.0314, -0.1066,\n",
       "          0.1401,  0.0314, -0.0314,  0.1945,  0.1192, -0.1359,  0.0544,  0.0063,\n",
       "         -0.0084,  0.1025,  0.1087, -0.0460,  0.0355, -0.2321, -0.0314, -0.0460,\n",
       "          0.0293, -0.2217, -0.2405,  0.1087, -0.0146, -0.0293,  0.0251, -0.0606,\n",
       "         -0.0209,  0.0857,  0.0230, -0.0774, -0.0481,  0.1192,  0.0063,  0.0836,\n",
       "          0.0021, -0.0878,  0.1882,  0.0836]], size=(10, 84),\n",
       "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
       "       scale=0.002091140951961279, zero_point=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.fc3.weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def float_to_fixed_point(value, total_bits, fractional_bits):\n",
    "    \"\"\"\n",
    "    Converts a floating-point value to a fixed-point representation.\n",
    "\n",
    "    Args:\n",
    "        value: The floating-point value to convert.\n",
    "        total_bits: The total number of bits in the fixed-point representation.\n",
    "        fractional_bits: The number of fractional bits.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The fixed-point representation as an integer.\n",
    "        - The scaling factor used (2**fractional_bits).\n",
    "    \"\"\"\n",
    "    scaling_factor = 2**fractional_bits\n",
    "    # Scale the value\n",
    "    scaled_value = value * scaling_factor\n",
    "\n",
    "    # Handle rounding (round to nearest integer)\n",
    "    rounded_value = int(round(scaled_value))\n",
    "\n",
    "    # Check for overflow\n",
    "    max_value = (2**(total_bits - 1)) - 1\n",
    "    min_value = -(2**(total_bits - 1))\n",
    "\n",
    "    if rounded_value > max_value or rounded_value < min_value:\n",
    "        raise OverflowError(f\"Value {value} cannot be represented in Q{total_bits}.{fractional_bits} format.\")\n",
    "    \n",
    "    # Clip the value to the range\n",
    "    clipped_value = max(min(rounded_value, max_value), min_value)\n",
    "\n",
    "    return clipped_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module quant params stored\n",
      "Module conv1 params stored\n",
      "Module conv2 params stored\n",
      "Module fc1 params stored\n",
      "Module fc2 params stored\n",
      "Module fc3 params stored\n"
     ]
    }
   ],
   "source": [
    "def save_quantized_model_params_fpga(model, file_prefix):\n",
    "    model_params = {}\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight'):\n",
    "            weight = module.weight().int_repr()\n",
    "            weight_raw = module.weight().dequantize()\n",
    "            weight_scale = module.weight().q_scale()\n",
    "            scale = module.scale\n",
    "            zero_point = module.zero_point\n",
    "\n",
    "            #convert scale to fixed_point\n",
    "            #scale_int = float_to_fixed_point(scale, total_bits = 16, fractional_bits = 4)\n",
    "\n",
    "            model_params[f'{name}_weight'] = weight.detach().cpu().numpy().astype(np.int8)\n",
    "            model_params[f'{name}_weight_raw'] = weight_raw.detach().cpu().numpy()\n",
    "            model_params[f'{name}_weight_scale'] = weight_scale\n",
    "            model_params[f'{name}_scale'] = scale\n",
    "            model_params[f'{name}_zero_point'] = zero_point\n",
    "\n",
    "            print(f\"Module {name} params stored\")\n",
    "        \n",
    "        elif hasattr(module, 'scale') and not hasattr(module, 'weight'):\n",
    "            scale = module.scale.item()\n",
    "            zero_point = module.zero_point.item()\n",
    "\n",
    "            #convert scale to fixed_point\n",
    "            #scale_int = float_to_fixed_point(scale, total_bits = 16, fractional_bits = 4)\n",
    "            model_params[f'{name}_scale'] = scale\n",
    "            model_params[f'{name}_zero_point'] = zero_point\n",
    "\n",
    "            print(f\"Module {name} params stored\")\n",
    "\n",
    "    # Save parameters as integers for FPGA use\n",
    "    np.savez(f'{file_prefix}_fpga_quantized_params.npz', **model_params)\n",
    "    return model_params\n",
    "\n",
    "# Save the fixed-point parameters for FPGA\n",
    "test_dict = save_quantized_model_params_fpga(quant_model, 'raw_quantized_model_fpga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(test_dict, \"parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef save_quantized_model_params_fpga(model, file_prefix):\\n    model_params = {}\\n\\n    for name, module in model.named_modules():\\n        if isinstance(module, torch.nn.quantized.Conv2d):\\n            weight = module.weight().int_repr()\\n            bias = module.bias() if module.bias() is not None else None\\n            scale = module.scale\\n            zero_point = module.zero_point\\n\\n            # Convert scale and zero-point to fixed-point\\n            scale_int, zero_point_int = save_fixed_point_params(scale, zero_point)\\n\\n            model_params[f'{name}_weight'] = weight.detach().cpu().numpy().astype(np.int8)\\n            if bias is not None:\\n                model_params[f'{name}_bias'] = bias.detach().cpu().numpy().astype(np.int32)  # Bias may need 32-bit\\n            model_params[f'{name}_scale'] = scale_int\\n            model_params[f'{name}_zero_point'] = zero_point_int\\n\\n        elif isinstance(module, torch.nn.quantized.Linear):\\n            weight = module.weight().int_repr()\\n            bias = module.bias() if module.bias() is not None else None\\n            scale = module.scale\\n            zero_point = module.zero_point\\n\\n            scale_int, zero_point_int = save_fixed_point_params(scale, zero_point)\\n\\n            model_params[f'{name}_weight'] = weight.detach().cpu().numpy().astype(np.int8)\\n            if bias is not None:\\n                model_params[f'{name}_bias'] = bias.detach().cpu().numpy().astype(np.int32)\\n            model_params[f'{name}_scale'] = scale_int\\n            model_params[f'{name}_zero_point'] = zero_point_int\\n\\n    # Save parameters as integers for FPGA use\\n    np.savez(f'{file_prefix}_fpga_quantized_params.npz', **model_params)\\n    return model_params\\n\\n# Save the fixed-point parameters for FPGA\\ntest_dict = save_quantized_model_params_fpga(quant_model, 'quantized_model_fpga')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def save_quantized_model_params_fpga(model, file_prefix):\n",
    "    model_params = {}\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.quantized.Conv2d):\n",
    "            weight = module.weight().int_repr()\n",
    "            bias = module.bias() if module.bias() is not None else None\n",
    "            scale = module.scale\n",
    "            zero_point = module.zero_point\n",
    "\n",
    "            # Convert scale and zero-point to fixed-point\n",
    "            scale_int, zero_point_int = save_fixed_point_params(scale, zero_point)\n",
    "\n",
    "            model_params[f'{name}_weight'] = weight.detach().cpu().numpy().astype(np.int8)\n",
    "            if bias is not None:\n",
    "                model_params[f'{name}_bias'] = bias.detach().cpu().numpy().astype(np.int32)  # Bias may need 32-bit\n",
    "            model_params[f'{name}_scale'] = scale_int\n",
    "            model_params[f'{name}_zero_point'] = zero_point_int\n",
    "\n",
    "        elif isinstance(module, torch.nn.quantized.Linear):\n",
    "            weight = module.weight().int_repr()\n",
    "            bias = module.bias() if module.bias() is not None else None\n",
    "            scale = module.scale\n",
    "            zero_point = module.zero_point\n",
    "\n",
    "            scale_int, zero_point_int = save_fixed_point_params(scale, zero_point)\n",
    "\n",
    "            model_params[f'{name}_weight'] = weight.detach().cpu().numpy().astype(np.int8)\n",
    "            if bias is not None:\n",
    "                model_params[f'{name}_bias'] = bias.detach().cpu().numpy().astype(np.int32)\n",
    "            model_params[f'{name}_scale'] = scale_int\n",
    "            model_params[f'{name}_zero_point'] = zero_point_int\n",
    "\n",
    "    # Save parameters as integers for FPGA use\n",
    "    np.savez(f'{file_prefix}_fpga_quantized_params.npz', **model_params)\n",
    "    return model_params\n",
    "\n",
    "# Save the fixed-point parameters for FPGA\n",
    "test_dict = save_quantized_model_params_fpga(quant_model, 'quantized_model_fpga')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original scale factor: 0.0127\n",
      "Scaled scale factor (Q12.4): 7\n",
      "Binary representation: 0b111\n"
     ]
    }
   ],
   "source": [
    "def scale_and_convert_to_q12_4(scale_factor, scaling_factor=4096):\n",
    "  \"\"\"Scales a scale factor and converts it to Q12.4 fixed-point format.\"\"\"\n",
    "\n",
    "  scaled_scale_factor = scale_factor * scaling_factor\n",
    "  # Convert to Q12.4 (assuming a function float_to_fixed_point exists)\n",
    "  scaled_scale_factor_q12_4 = float_to_fixed_point(scaled_scale_factor, 16, 4) \n",
    "  #17 total bits, 4 fractional bits\n",
    "  return scaled_scale_factor_q12_4\n",
    "\n",
    "# Example usage\n",
    "original_scale_factor = 0.0127\n",
    "scaling_factor = 4096  # 2^12\n",
    "\n",
    "scaled_scale_factor_q12_4 = scale_and_convert_to_q12_4(original_scale_factor, scaling_factor=32)\n",
    "\n",
    "print(f\"Original scale factor: {original_scale_factor}\")\n",
    "print(f\"Scaled scale factor (Q12.4): {scaled_scale_factor_q12_4}\")\n",
    "print(f\"Binary representation: {bin(scaled_scale_factor_q12_4 & (2**16 -1))}\") # Mask to get only 16 bits for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_scale': 0.012722019106149673,\n",
       " 'quant_zero_point': 33,\n",
       " 'conv1_weight': array([[[[  32,   41,   -5,  -45,   60],\n",
       "          [ -28,   57,  -10,    1,   68],\n",
       "          [  88,   17,  -74,  -68,   61],\n",
       "          [  31,   41,  -57,  -98,  -34],\n",
       "          [  10,   36,   51,    4,   40]]],\n",
       " \n",
       " \n",
       "        [[[ -49,  -27,  -16,   27,   19],\n",
       "          [ -87,  -16,   86,  -15,   -5],\n",
       "          [ -57,  -78,   80,   51,    0],\n",
       "          [ -47,  -56,   92,   17,  -26],\n",
       "          [-114,   -1,   41,   75,  -16]]],\n",
       " \n",
       " \n",
       "        [[[ -14,   50,  -28,   46,  -21],\n",
       "          [ -46,   40,   73,   41,   56],\n",
       "          [  64,   73,  -45,  -18,   24],\n",
       "          [ -51,   11,   51,   93,   43],\n",
       "          [  51,   44,  -58,   43,   79]]],\n",
       " \n",
       " \n",
       "        [[[  48,   40,   97,   80,   76],\n",
       "          [  63,  -16,  -29,   48,   66],\n",
       "          [ -15,   48,  -92,  -26,   54],\n",
       "          [ -38,  -73,  -18,  -99,  -23],\n",
       "          [ -84,  -25,    9,  -64,  -42]]],\n",
       " \n",
       " \n",
       "        [[[ -41,   87,   38,   -5,   12],\n",
       "          [   6,   34,   76,   -8,  -77],\n",
       "          [  11,   19,   80,   55,  -34],\n",
       "          [  -8,   73,   34,  -18,  -62],\n",
       "          [  62,   64,    5,  -21,   -3]]],\n",
       " \n",
       " \n",
       "        [[[  11,  -85, -108, -102,  -26],\n",
       "          [  35,   23,   24, -114, -128],\n",
       "          [  86,   18,   26,  -14,  -61],\n",
       "          [  55,   25,  103,   55,  111],\n",
       "          [ -49,   56,   64,   85,   62]]]], dtype=int8),\n",
       " 'conv1_weight_raw': array([[[[ 0.09826294,  0.12589939, -0.01535358, -0.13818225,\n",
       "            0.18424301],\n",
       "          [-0.08598007,  0.17503086, -0.03070717,  0.00307072,\n",
       "            0.20880874],\n",
       "          [ 0.27022308,  0.05220218, -0.22723304, -0.20880874,\n",
       "            0.18731372],\n",
       "          [ 0.09519222,  0.12589939, -0.17503086, -0.30093023,\n",
       "           -0.10440437],\n",
       "          [ 0.03070717,  0.1105458 ,  0.15660655,  0.01228287,\n",
       "            0.12282867]]],\n",
       " \n",
       " \n",
       "        [[[-0.15046512, -0.08290935, -0.04913147,  0.08290935,\n",
       "            0.05834362],\n",
       "          [-0.26715237, -0.04913147,  0.26408163, -0.04606075,\n",
       "           -0.01535358],\n",
       "          [-0.17503086, -0.2395159 ,  0.24565734,  0.15660655,\n",
       "            0.        ],\n",
       "          [-0.14432369, -0.17196015,  0.28250593,  0.05220218,\n",
       "           -0.07983863],\n",
       "          [-0.3500617 , -0.00307072,  0.12589939,  0.23030375,\n",
       "           -0.04913147]]],\n",
       " \n",
       " \n",
       "        [[[-0.04299004,  0.15353584, -0.08598007,  0.14125296,\n",
       "           -0.06448505],\n",
       "          [-0.14125296,  0.12282867,  0.22416233,  0.12589939,\n",
       "            0.17196015],\n",
       "          [ 0.19652587,  0.22416233, -0.13818225, -0.0552729 ,\n",
       "            0.0736972 ],\n",
       "          [-0.15660655,  0.03377789,  0.15660655,  0.28557667,\n",
       "            0.13204081],\n",
       "          [ 0.15660655,  0.13511154, -0.17810157,  0.13204081,\n",
       "            0.24258663]]],\n",
       " \n",
       " \n",
       "        [[[ 0.1473944 ,  0.12282867,  0.29785952,  0.24565734,\n",
       "            0.23337448],\n",
       "          [ 0.19345516, -0.04913147, -0.08905078,  0.1473944 ,\n",
       "            0.20266731],\n",
       "          [-0.04606075,  0.1473944 , -0.28250593, -0.07983863,\n",
       "            0.1658187 ],\n",
       "          [-0.11668724, -0.22416233, -0.0552729 , -0.30400094,\n",
       "           -0.07062648],\n",
       "          [-0.2579402 , -0.07676792,  0.02763645, -0.19652587,\n",
       "           -0.1289701 ]]],\n",
       " \n",
       " \n",
       "        [[[-0.12589939,  0.26715237,  0.11668724, -0.01535358,\n",
       "            0.0368486 ],\n",
       "          [ 0.0184243 ,  0.10440437,  0.23337448, -0.02456573,\n",
       "           -0.23644519],\n",
       "          [ 0.03377789,  0.05834362,  0.24565734,  0.16888942,\n",
       "           -0.10440437],\n",
       "          [-0.02456573,  0.22416233,  0.10440437, -0.0552729 ,\n",
       "           -0.19038443],\n",
       "          [ 0.19038443,  0.19652587,  0.01535358, -0.06448505,\n",
       "           -0.00921215]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03377789, -0.26101092, -0.3316374 , -0.3132131 ,\n",
       "           -0.07983863],\n",
       "          [ 0.10747509,  0.07062648,  0.0736972 , -0.3500617 ,\n",
       "           -0.39305174],\n",
       "          [ 0.26408163,  0.0552729 ,  0.07983863, -0.04299004,\n",
       "           -0.18731372],\n",
       "          [ 0.16888942,  0.07676792,  0.31628382,  0.16888942,\n",
       "            0.34084955],\n",
       "          [-0.15046512,  0.17196015,  0.19652587,  0.26101092,\n",
       "            0.19038443]]]], dtype=float32),\n",
       " 'conv1_weight_scale': 0.0030707167461514473,\n",
       " 'conv1_scale': 0.027446914464235306,\n",
       " 'conv1_zero_point': 0,\n",
       " 'conv2_weight': array([[[[  15,   21,   47,   -9,   16],\n",
       "          [ -28,    5,   25,   60,  -38],\n",
       "          [  20,   37,   42,    1,   12],\n",
       "          [  -8,   -8,   53,    6,   42],\n",
       "          [  -1,   -2,   35,   46,   22]],\n",
       " \n",
       "         [[  14,    3,  -16,   -4,   19],\n",
       "          [  -6,   45,   82,    1,   48],\n",
       "          [ -33,   53,   58,   26,   21],\n",
       "          [ -84,   30,   12,   34,  -21],\n",
       "          [ -35,   30,   -7,   17,  -54]],\n",
       " \n",
       "         [[   0,    1,  -31,  -17,    0],\n",
       "          [  16,   55,   28,  -23,   14],\n",
       "          [  14,   63,   17,  -23,  -61],\n",
       "          [ -24,   42,   45,  -13,    7],\n",
       "          [  13,   19,   19,   -9,  -29]],\n",
       " \n",
       "         [[ -18,  -55,  -72,  -44,    2],\n",
       "          [  11, -119,  -47,  -20,  -45],\n",
       "          [  42,  -10, -109,  -28,  -36],\n",
       "          [  13,   35,  -41,  -77,  -22],\n",
       "          [ -17,   16,   24,  -37,  -42]],\n",
       " \n",
       "         [[ -35,   34,   38,  -21,   20],\n",
       "          [ -21,   35,   51,    2,   -2],\n",
       "          [  27,   66,    9,   28,  -36],\n",
       "          [ -16,   38,   36,    7,  -49],\n",
       "          [ -49,    2,   41,   37,   -7]],\n",
       " \n",
       "         [[ -22,   -7,   -5,  -87,  -10],\n",
       "          [  -7,    5,   -1,  -17,  -32],\n",
       "          [ -62,  -48,   61,   18,  -56],\n",
       "          [ -84, -111,   31,   47,  -11],\n",
       "          [ -66,  -53,  -40,   63,   26]]],\n",
       " \n",
       " \n",
       "        [[[ -39,  -45,  -35,  -29,  -33],\n",
       "          [  22,    6,   18,  -25,  -36],\n",
       "          [ -38,    9,   12,   11,    3],\n",
       "          [  34,   26,    4,   39,   21],\n",
       "          [ -45,  -22,   -8,  -10,  -38]],\n",
       " \n",
       "         [[   9,    5,   41,   -3,   20],\n",
       "          [   6,   12,   44,   -8,   10],\n",
       "          [ -29,  -48,   36,  -63,  -80],\n",
       "          [ -29,    2,  -18,  -52,  -84],\n",
       "          [  47,   21,   -4,  -42,  -82]],\n",
       " \n",
       "         [[ -17,   29,  -47,  -25,  -42],\n",
       "          [ -10,   46,   -1,  -23,   22],\n",
       "          [  39,   34,   44,   51,   41],\n",
       "          [  -3,   45,   58,    2,   15],\n",
       "          [ -15,    2,  -49,  -35,  -65]],\n",
       " \n",
       "         [[ -43,  -32,  -99,  -35,  -56],\n",
       "          [ -41,  -69,  -63,  -65,    7],\n",
       "          [ -21,  -35,  -16,  -34,   40],\n",
       "          [  19,   50,   61,   93,   67],\n",
       "          [  -5,   -7,   23,   69,   68]],\n",
       " \n",
       "         [[  -4,   26,   47,    3,  -66],\n",
       "          [  -1,    4,   10,   -7,   -9],\n",
       "          [ -20,   -4,   -6,  -13,  -17],\n",
       "          [  25,   14,  -32,    4,   -1],\n",
       "          [  -2,   -2,  -47,  -63,  -44]],\n",
       " \n",
       "         [[ -68,  -79,  -90,  -79,  -86],\n",
       "          [ -38,  -17,   35,   30,   58],\n",
       "          [  39,   23,   82,   72,   33],\n",
       "          [ -42,  -21,   13,  -23,   16],\n",
       "          [ -52,  -14,   -4,  -36,  -68]]],\n",
       " \n",
       " \n",
       "        [[[ -73,   -8,    8,   30,   16],\n",
       "          [  -7,   35,   -7,  -10,  -12],\n",
       "          [  35,    4,   -8,   49,   -8],\n",
       "          [   9,   34,  -10,   11,  -20],\n",
       "          [ -34,  -20,  -35,   -7,   -1]],\n",
       " \n",
       "         [[  18,   11,  -18,    2,  -28],\n",
       "          [  58,  -20,  -12,  -19,   22],\n",
       "          [  56,  -38,  -14,   25,  -17],\n",
       "          [  16,  -29,  -73,  -52,  -40],\n",
       "          [ -36,  -35,  -80,  -56,  -12]],\n",
       " \n",
       "         [[ -48,  -15,  -11,   11,  -13],\n",
       "          [ -42,   -2,    9,   10,  -29],\n",
       "          [  32,   10,   28,   31,   42],\n",
       "          [   2,  -20,   21,    5,   39],\n",
       "          [ -57,  -63,    5,   48,  -12]],\n",
       " \n",
       "         [[  30,  -26,  -61,  -16,  -51],\n",
       "          [  23,   13,   34,   24,  -55],\n",
       "          [  16,   -5,  -13,  -19,  -53],\n",
       "          [  67,   65,   40,  -12,  -63],\n",
       "          [  42,   48,   49,    4,    5]],\n",
       " \n",
       "         [[ -38,   15,  -20,    4,    9],\n",
       "          [  16,  -41,  -42,   -6,  -12],\n",
       "          [   7,   12,   28,   16,   46],\n",
       "          [ -10,  -41,  -12,    6,   41],\n",
       "          [ -35,  -19,   -5,   29,   20]],\n",
       " \n",
       "         [[ -25,    3,   19,    0,   -8],\n",
       "          [  -7,    4,  -14,   37,  -11],\n",
       "          [  55,   52,    1,   24,   45],\n",
       "          [  15,   24,  -19,   44,   66],\n",
       "          [ -48,  -59,  -73,  -21,  -31]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[ -76,  -16,   16,   22,   28],\n",
       "          [ -34,   66,   63,   31,  -38],\n",
       "          [  10,   65,   18,   13,  -47],\n",
       "          [  49,   22,   16,   26,  -13],\n",
       "          [  21,    4,   20,  -14,  -40]],\n",
       " \n",
       "         [[  16,   23,   33,  -23,    9],\n",
       "          [  93,   77,   20,  -57,    8],\n",
       "          [  68,   49,   38,   -9,  -61],\n",
       "          [  69,   32,   20,  -52,  -43],\n",
       "          [ -17,   20,  -24,  -20,  -42]],\n",
       " \n",
       "         [[  31,   58,   43,   17,  -18],\n",
       "          [ -20,   15,   37,  -33,   -6],\n",
       "          [  -2,   34,   -9,   -3,  -37],\n",
       "          [ -18,   30,  -31,  -30,  -88],\n",
       "          [ -22,  -19,  -12,   17,  -59]],\n",
       " \n",
       "         [[ -95,  -88,    6,   65,   36],\n",
       "          [ -74,   -7,   37,   72,   68],\n",
       "          [ -23,  -38,    2,    2,   28],\n",
       "          [  21,  -35,    9,  -12,   66],\n",
       "          [   3,    4,   29,   -8,   -1]],\n",
       " \n",
       "         [[  -5,   26,   34,    7,    1],\n",
       "          [  30,   36,   27,  -13,  -14],\n",
       "          [   8,   -7,  -31,  -39,  -18],\n",
       "          [   2,   25,  -16,  -16,  -72],\n",
       "          [ -16,  -30,  -36,   -1,  -24]],\n",
       " \n",
       "         [[   8,  -27,  -55,  -53,  -27],\n",
       "          [ -72,  -59,  -67,  -20,   31],\n",
       "          [-118,  -26,   34,   16,  -29],\n",
       "          [ -57,   64,   13,   61,    2],\n",
       "          [  28,   64,   57,   53,   29]]],\n",
       " \n",
       " \n",
       "        [[[  16,   22,   -9,  -14,   22],\n",
       "          [  54,   33,  -50,  -19,   44],\n",
       "          [  21,  -46,  -60,  -17,    0],\n",
       "          [ -10,   -5,   10,   66,   23],\n",
       "          [  47,   39,   49,   30,   -1]],\n",
       " \n",
       "         [[  -2,    5,   12,   -6,   63],\n",
       "          [   2,  -84,   33,   67,   81],\n",
       "          [   3,  -69,   51,   68,   10],\n",
       "          [ -25,   18,   35,   50,   18],\n",
       "          [ -30,  -21,  -41,   31,  -29]],\n",
       " \n",
       "         [[  -8,  -39,  -26,   67,    6],\n",
       "          [  -2,  -20,   56,   40,   50],\n",
       "          [ -17,   -8,   -3,    4,   27],\n",
       "          [   5,   45,   64,   35,   -2],\n",
       "          [  14,   38,    4,  -17,  -49]],\n",
       " \n",
       "         [[ -65,  -10,   18,   10,   -6],\n",
       "          [  57,  -32,  -58,  -48,  -36],\n",
       "          [ -15,  -56,  -54,  -77,   12],\n",
       "          [  -5,  -71,  -26,   26,   -2],\n",
       "          [  -1,   53,   21,   30,  -47]],\n",
       " \n",
       "         [[  20,  -12,  -40,    9,   48],\n",
       "          [  65,   -5,  -60,   74,   34],\n",
       "          [ -22,  -27,    4,   85,   63],\n",
       "          [ -32,    3,   40,   73,    5],\n",
       "          [ -28,   51,   47,   17,  -45]],\n",
       " \n",
       "         [[  73,  -25, -104,   23,    1],\n",
       "          [ -24,  -53,  -51,    7,  -83],\n",
       "          [  28,   12,  -25,  -64,  -62],\n",
       "          [  33,  -34,  -35,  -45,  -37],\n",
       "          [  36,  -16,  -68,  -63,  -31]]],\n",
       " \n",
       " \n",
       "        [[[  58,  -21,   59,   38,  -22],\n",
       "          [  24,   22,   41,   14,  -29],\n",
       "          [  -8,    0,   30,   43,   10],\n",
       "          [  20,   -3,  -18,   35,  -21],\n",
       "          [   6,  -50,   -5,  -21,    5]],\n",
       " \n",
       "         [[ -90,   44,    1,   -9,   35],\n",
       "          [ -56,  -35,  -71,  -35,    0],\n",
       "          [ -52,  -64,  -41,  -76,  -19],\n",
       "          [ -45,  -39,   18,  -17,  -14],\n",
       "          [  53,   48,   44,   21,   -2]],\n",
       " \n",
       "         [[   8,   21,   39,  -30,   -5],\n",
       "          [  22,   43,   13,   -9,    5],\n",
       "          [  39,   13,   15,  -14,   42],\n",
       "          [  22,   14,  -42,  -37,   30],\n",
       "          [ -66,   10,  -19,   16,   24]],\n",
       " \n",
       "         [[  36,   -7,  -86,  -91,  -47],\n",
       "          [  -7,   43,   37,  -36,  -54],\n",
       "          [   4,  -13,   -4,   76,   59],\n",
       "          [   3,   40,   47,   18,   37],\n",
       "          [  30,  -12,  -54,  -25,   18]],\n",
       " \n",
       "         [[  -7,   46,    5,    1,   13],\n",
       "          [ -47,   -7,  -11,   17,   24],\n",
       "          [   8,  -10,  -15,   -5,   27],\n",
       "          [  20,   16,  -31,  -33,  -18],\n",
       "          [  16,  -20,   10,  -42,    4]],\n",
       " \n",
       "         [[ -46,  -30,   49,   44,   13],\n",
       "          [  56,   62,   57,   60,   60],\n",
       "          [  28,   24,    4,  -14,   34],\n",
       "          [ -64,  -71,  -51,  -18,  -57],\n",
       "          [ -66,  -23,   22,  -20,  -40]]]], dtype=int8),\n",
       " 'conv2_weight_raw': array([[[[ 0.03240328,  0.0453646 ,  0.10153028, -0.01944197,\n",
       "            0.0345635 ],\n",
       "          [-0.06048613,  0.01080109,  0.05400547,  0.12961313,\n",
       "           -0.08208831],\n",
       "          [ 0.04320437,  0.07992809,  0.09072919,  0.00216022,\n",
       "            0.02592263],\n",
       "          [-0.01728175, -0.01728175,  0.1144916 ,  0.01296131,\n",
       "            0.09072919],\n",
       "          [-0.00216022, -0.00432044,  0.07560766,  0.09937006,\n",
       "            0.04752481]],\n",
       " \n",
       "         [[ 0.03024306,  0.00648066, -0.0345635 , -0.00864088,\n",
       "            0.04104416],\n",
       "          [-0.01296131,  0.09720985,  0.17713794,  0.00216022,\n",
       "            0.10369051],\n",
       "          [-0.07128722,  0.1144916 ,  0.12529269,  0.05616569,\n",
       "            0.0453646 ],\n",
       "          [-0.18145838,  0.06480657,  0.02592263,  0.07344744,\n",
       "           -0.0453646 ],\n",
       "          [-0.07560766,  0.06480657, -0.01512153,  0.03672372,\n",
       "           -0.11665181]],\n",
       " \n",
       "         [[ 0.        ,  0.00216022, -0.06696678, -0.03672372,\n",
       "            0.        ],\n",
       "          [ 0.0345635 ,  0.11881203,  0.06048613, -0.04968503,\n",
       "            0.03024306],\n",
       "          [ 0.03024306,  0.13609378,  0.03672372, -0.04968503,\n",
       "           -0.13177335],\n",
       "          [-0.05184525,  0.09072919,  0.09720985, -0.02808284,\n",
       "            0.01512153],\n",
       "          [ 0.02808284,  0.04104416,  0.04104416, -0.01944197,\n",
       "           -0.06264634]],\n",
       " \n",
       "         [[-0.03888394, -0.11881203, -0.15553576, -0.09504963,\n",
       "            0.00432044],\n",
       "          [ 0.02376241, -0.25706604, -0.10153028, -0.04320437,\n",
       "           -0.09720985],\n",
       "          [ 0.09072919, -0.02160219, -0.23546384, -0.06048613,\n",
       "           -0.07776788],\n",
       "          [ 0.02808284,  0.07560766, -0.08856897, -0.16633685,\n",
       "           -0.04752481],\n",
       "          [-0.03672372,  0.0345635 ,  0.05184525, -0.07992809,\n",
       "           -0.09072919]],\n",
       " \n",
       "         [[-0.07560766,  0.07344744,  0.08208831, -0.0453646 ,\n",
       "            0.04320437],\n",
       "          [-0.0453646 ,  0.07560766,  0.11017115,  0.00432044,\n",
       "           -0.00432044],\n",
       "          [ 0.05832591,  0.14257444,  0.01944197,  0.06048613,\n",
       "           -0.07776788],\n",
       "          [-0.0345635 ,  0.08208831,  0.07776788,  0.01512153,\n",
       "           -0.10585072],\n",
       "          [-0.10585072,  0.00432044,  0.08856897,  0.07992809,\n",
       "           -0.01512153]],\n",
       " \n",
       "         [[-0.04752481, -0.01512153, -0.01080109, -0.18793903,\n",
       "           -0.02160219],\n",
       "          [-0.01512153,  0.01080109, -0.00216022, -0.03672372,\n",
       "           -0.069127  ],\n",
       "          [-0.13393356, -0.10369051,  0.13177335,  0.03888394,\n",
       "           -0.12097225],\n",
       "          [-0.18145838, -0.23978429,  0.06696678,  0.10153028,\n",
       "           -0.02376241],\n",
       "          [-0.14257444, -0.1144916 , -0.08640875,  0.13609378,\n",
       "            0.05616569]]],\n",
       " \n",
       " \n",
       "        [[[-0.08424854, -0.09720985, -0.07560766, -0.06264634,\n",
       "           -0.07128722],\n",
       "          [ 0.04752481,  0.01296131,  0.03888394, -0.05400547,\n",
       "           -0.07776788],\n",
       "          [-0.08208831,  0.01944197,  0.02592263,  0.02376241,\n",
       "            0.00648066],\n",
       "          [ 0.07344744,  0.05616569,  0.00864088,  0.08424854,\n",
       "            0.0453646 ],\n",
       "          [-0.09720985, -0.04752481, -0.01728175, -0.02160219,\n",
       "           -0.08208831]],\n",
       " \n",
       "         [[ 0.01944197,  0.01080109,  0.08856897, -0.00648066,\n",
       "            0.04320437],\n",
       "          [ 0.01296131,  0.02592263,  0.09504963, -0.01728175,\n",
       "            0.02160219],\n",
       "          [-0.06264634, -0.10369051,  0.07776788, -0.13609378,\n",
       "           -0.1728175 ],\n",
       "          [-0.06264634,  0.00432044, -0.03888394, -0.11233138,\n",
       "           -0.18145838],\n",
       "          [ 0.10153028,  0.0453646 , -0.00864088, -0.09072919,\n",
       "           -0.17713794]],\n",
       " \n",
       "         [[-0.03672372,  0.06264634, -0.10153028, -0.05400547,\n",
       "           -0.09072919],\n",
       "          [-0.02160219,  0.09937006, -0.00216022, -0.04968503,\n",
       "            0.04752481],\n",
       "          [ 0.08424854,  0.07344744,  0.09504963,  0.11017115,\n",
       "            0.08856897],\n",
       "          [-0.00648066,  0.09720985,  0.12529269,  0.00432044,\n",
       "            0.03240328],\n",
       "          [-0.03240328,  0.00432044, -0.10585072, -0.07560766,\n",
       "           -0.14041422]],\n",
       " \n",
       "         [[-0.09288941, -0.069127  , -0.21386166, -0.07560766,\n",
       "           -0.12097225],\n",
       "          [-0.08856897, -0.1490551 , -0.13609378, -0.14041422,\n",
       "            0.01512153],\n",
       "          [-0.0453646 , -0.07560766, -0.0345635 , -0.07344744,\n",
       "            0.08640875],\n",
       "          [ 0.04104416,  0.10801094,  0.13177335,  0.20090035,\n",
       "            0.14473465],\n",
       "          [-0.01080109, -0.01512153,  0.04968503,  0.1490551 ,\n",
       "            0.14689487]],\n",
       " \n",
       "         [[-0.00864088,  0.05616569,  0.10153028,  0.00648066,\n",
       "           -0.14257444],\n",
       "          [-0.00216022,  0.00864088,  0.02160219, -0.01512153,\n",
       "           -0.01944197],\n",
       "          [-0.04320437, -0.00864088, -0.01296131, -0.02808284,\n",
       "           -0.03672372],\n",
       "          [ 0.05400547,  0.03024306, -0.069127  ,  0.00864088,\n",
       "           -0.00216022],\n",
       "          [-0.00432044, -0.00432044, -0.10153028, -0.13609378,\n",
       "           -0.09504963]],\n",
       " \n",
       "         [[-0.14689487, -0.17065728, -0.1944197 , -0.17065728,\n",
       "           -0.18577881],\n",
       "          [-0.08208831, -0.03672372,  0.07560766,  0.06480657,\n",
       "            0.12529269],\n",
       "          [ 0.08424854,  0.04968503,  0.17713794,  0.15553576,\n",
       "            0.07128722],\n",
       "          [-0.09072919, -0.0453646 ,  0.02808284, -0.04968503,\n",
       "            0.0345635 ],\n",
       "          [-0.11233138, -0.03024306, -0.00864088, -0.07776788,\n",
       "           -0.14689487]]],\n",
       " \n",
       " \n",
       "        [[[-0.15769596, -0.01728175,  0.01728175,  0.06480657,\n",
       "            0.0345635 ],\n",
       "          [-0.01512153,  0.07560766, -0.01512153, -0.02160219,\n",
       "           -0.02592263],\n",
       "          [ 0.07560766,  0.00864088, -0.01728175,  0.10585072,\n",
       "           -0.01728175],\n",
       "          [ 0.01944197,  0.07344744, -0.02160219,  0.02376241,\n",
       "           -0.04320437],\n",
       "          [-0.07344744, -0.04320437, -0.07560766, -0.01512153,\n",
       "           -0.00216022]],\n",
       " \n",
       "         [[ 0.03888394,  0.02376241, -0.03888394,  0.00432044,\n",
       "           -0.06048613],\n",
       "          [ 0.12529269, -0.04320437, -0.02592263, -0.04104416,\n",
       "            0.04752481],\n",
       "          [ 0.12097225, -0.08208831, -0.03024306,  0.05400547,\n",
       "           -0.03672372],\n",
       "          [ 0.0345635 , -0.06264634, -0.15769596, -0.11233138,\n",
       "           -0.08640875],\n",
       "          [-0.07776788, -0.07560766, -0.1728175 , -0.12097225,\n",
       "           -0.02592263]],\n",
       " \n",
       "         [[-0.10369051, -0.03240328, -0.02376241,  0.02376241,\n",
       "           -0.02808284],\n",
       "          [-0.09072919, -0.00432044,  0.01944197,  0.02160219,\n",
       "           -0.06264634],\n",
       "          [ 0.069127  ,  0.02160219,  0.06048613,  0.06696678,\n",
       "            0.09072919],\n",
       "          [ 0.00432044, -0.04320437,  0.0453646 ,  0.01080109,\n",
       "            0.08424854],\n",
       "          [-0.12313247, -0.13609378,  0.01080109,  0.10369051,\n",
       "           -0.02592263]],\n",
       " \n",
       "         [[ 0.06480657, -0.05616569, -0.13177335, -0.0345635 ,\n",
       "           -0.11017115],\n",
       "          [ 0.04968503,  0.02808284,  0.07344744,  0.05184525,\n",
       "           -0.11881203],\n",
       "          [ 0.0345635 , -0.01080109, -0.02808284, -0.04104416,\n",
       "           -0.1144916 ],\n",
       "          [ 0.14473465,  0.14041422,  0.08640875, -0.02592263,\n",
       "           -0.13609378],\n",
       "          [ 0.09072919,  0.10369051,  0.10585072,  0.00864088,\n",
       "            0.01080109]],\n",
       " \n",
       "         [[-0.08208831,  0.03240328, -0.04320437,  0.00864088,\n",
       "            0.01944197],\n",
       "          [ 0.0345635 , -0.08856897, -0.09072919, -0.01296131,\n",
       "           -0.02592263],\n",
       "          [ 0.01512153,  0.02592263,  0.06048613,  0.0345635 ,\n",
       "            0.09937006],\n",
       "          [-0.02160219, -0.08856897, -0.02592263,  0.01296131,\n",
       "            0.08856897],\n",
       "          [-0.07560766, -0.04104416, -0.01080109,  0.06264634,\n",
       "            0.04320437]],\n",
       " \n",
       "         [[-0.05400547,  0.00648066,  0.04104416,  0.        ,\n",
       "           -0.01728175],\n",
       "          [-0.01512153,  0.00864088, -0.03024306,  0.07992809,\n",
       "           -0.02376241],\n",
       "          [ 0.11881203,  0.11233138,  0.00216022,  0.05184525,\n",
       "            0.09720985],\n",
       "          [ 0.03240328,  0.05184525, -0.04104416,  0.09504963,\n",
       "            0.14257444],\n",
       "          [-0.10369051, -0.12745291, -0.15769596, -0.0453646 ,\n",
       "           -0.06696678]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[-0.16417663, -0.0345635 ,  0.0345635 ,  0.04752481,\n",
       "            0.06048613],\n",
       "          [-0.07344744,  0.14257444,  0.13609378,  0.06696678,\n",
       "           -0.08208831],\n",
       "          [ 0.02160219,  0.14041422,  0.03888394,  0.02808284,\n",
       "           -0.10153028],\n",
       "          [ 0.10585072,  0.04752481,  0.0345635 ,  0.05616569,\n",
       "           -0.02808284],\n",
       "          [ 0.0453646 ,  0.00864088,  0.04320437, -0.03024306,\n",
       "           -0.08640875]],\n",
       " \n",
       "         [[ 0.0345635 ,  0.04968503,  0.07128722, -0.04968503,\n",
       "            0.01944197],\n",
       "          [ 0.20090035,  0.16633685,  0.04320437, -0.12313247,\n",
       "            0.01728175],\n",
       "          [ 0.14689487,  0.10585072,  0.08208831, -0.01944197,\n",
       "           -0.13177335],\n",
       "          [ 0.1490551 ,  0.069127  ,  0.04320437, -0.11233138,\n",
       "           -0.09288941],\n",
       "          [-0.03672372,  0.04320437, -0.05184525, -0.04320437,\n",
       "           -0.09072919]],\n",
       " \n",
       "         [[ 0.06696678,  0.12529269,  0.09288941,  0.03672372,\n",
       "           -0.03888394],\n",
       "          [-0.04320437,  0.03240328,  0.07992809, -0.07128722,\n",
       "           -0.01296131],\n",
       "          [-0.00432044,  0.07344744, -0.01944197, -0.00648066,\n",
       "           -0.07992809],\n",
       "          [-0.03888394,  0.06480657, -0.06696678, -0.06480657,\n",
       "           -0.19009925],\n",
       "          [-0.04752481, -0.04104416, -0.02592263,  0.03672372,\n",
       "           -0.12745291]],\n",
       " \n",
       "         [[-0.20522079, -0.19009925,  0.01296131,  0.14041422,\n",
       "            0.07776788],\n",
       "          [-0.15985619, -0.01512153,  0.07992809,  0.15553576,\n",
       "            0.14689487],\n",
       "          [-0.04968503, -0.08208831,  0.00432044,  0.00432044,\n",
       "            0.06048613],\n",
       "          [ 0.0453646 , -0.07560766,  0.01944197, -0.02592263,\n",
       "            0.14257444],\n",
       "          [ 0.00648066,  0.00864088,  0.06264634, -0.01728175,\n",
       "           -0.00216022]],\n",
       " \n",
       "         [[-0.01080109,  0.05616569,  0.07344744,  0.01512153,\n",
       "            0.00216022],\n",
       "          [ 0.06480657,  0.07776788,  0.05832591, -0.02808284,\n",
       "           -0.03024306],\n",
       "          [ 0.01728175, -0.01512153, -0.06696678, -0.08424854,\n",
       "           -0.03888394],\n",
       "          [ 0.00432044,  0.05400547, -0.0345635 , -0.0345635 ,\n",
       "           -0.15553576],\n",
       "          [-0.0345635 , -0.06480657, -0.07776788, -0.00216022,\n",
       "           -0.05184525]],\n",
       " \n",
       "         [[ 0.01728175, -0.05832591, -0.11881203, -0.1144916 ,\n",
       "           -0.05832591],\n",
       "          [-0.15553576, -0.12745291, -0.14473465, -0.04320437,\n",
       "            0.06696678],\n",
       "          [-0.25490582, -0.05616569,  0.07344744,  0.0345635 ,\n",
       "           -0.06264634],\n",
       "          [-0.12313247,  0.138254  ,  0.02808284,  0.13177335,\n",
       "            0.00432044],\n",
       "          [ 0.06048613,  0.138254  ,  0.12313247,  0.1144916 ,\n",
       "            0.06264634]]],\n",
       " \n",
       " \n",
       "        [[[ 0.0345635 ,  0.04752481, -0.01944197, -0.03024306,\n",
       "            0.04752481],\n",
       "          [ 0.11665181,  0.07128722, -0.10801094, -0.04104416,\n",
       "            0.09504963],\n",
       "          [ 0.0453646 , -0.09937006, -0.12961313, -0.03672372,\n",
       "            0.        ],\n",
       "          [-0.02160219, -0.01080109,  0.02160219,  0.14257444,\n",
       "            0.04968503],\n",
       "          [ 0.10153028,  0.08424854,  0.10585072,  0.06480657,\n",
       "           -0.00216022]],\n",
       " \n",
       "         [[-0.00432044,  0.01080109,  0.02592263, -0.01296131,\n",
       "            0.13609378],\n",
       "          [ 0.00432044, -0.18145838,  0.07128722,  0.14473465,\n",
       "            0.17497772],\n",
       "          [ 0.00648066, -0.1490551 ,  0.11017115,  0.14689487,\n",
       "            0.02160219],\n",
       "          [-0.05400547,  0.03888394,  0.07560766,  0.10801094,\n",
       "            0.03888394],\n",
       "          [-0.06480657, -0.0453646 , -0.08856897,  0.06696678,\n",
       "           -0.06264634]],\n",
       " \n",
       "         [[-0.01728175, -0.08424854, -0.05616569,  0.14473465,\n",
       "            0.01296131],\n",
       "          [-0.00432044, -0.04320437,  0.12097225,  0.08640875,\n",
       "            0.10801094],\n",
       "          [-0.03672372, -0.01728175, -0.00648066,  0.00864088,\n",
       "            0.05832591],\n",
       "          [ 0.01080109,  0.09720985,  0.138254  ,  0.07560766,\n",
       "           -0.00432044],\n",
       "          [ 0.03024306,  0.08208831,  0.00864088, -0.03672372,\n",
       "           -0.10585072]],\n",
       " \n",
       "         [[-0.14041422, -0.02160219,  0.03888394,  0.02160219,\n",
       "           -0.01296131],\n",
       "          [ 0.12313247, -0.069127  , -0.12529269, -0.10369051,\n",
       "           -0.07776788],\n",
       "          [-0.03240328, -0.12097225, -0.11665181, -0.16633685,\n",
       "            0.02592263],\n",
       "          [-0.01080109, -0.15337554, -0.05616569,  0.05616569,\n",
       "           -0.00432044],\n",
       "          [-0.00216022,  0.1144916 ,  0.0453646 ,  0.06480657,\n",
       "           -0.10153028]],\n",
       " \n",
       "         [[ 0.04320437, -0.02592263, -0.08640875,  0.01944197,\n",
       "            0.10369051],\n",
       "          [ 0.14041422, -0.01080109, -0.12961313,  0.15985619,\n",
       "            0.07344744],\n",
       "          [-0.04752481, -0.05832591,  0.00864088,  0.18361859,\n",
       "            0.13609378],\n",
       "          [-0.069127  ,  0.00648066,  0.08640875,  0.15769596,\n",
       "            0.01080109],\n",
       "          [-0.06048613,  0.11017115,  0.10153028,  0.03672372,\n",
       "           -0.09720985]],\n",
       " \n",
       "         [[ 0.15769596, -0.05400547, -0.22466275,  0.04968503,\n",
       "            0.00216022],\n",
       "          [-0.05184525, -0.1144916 , -0.11017115,  0.01512153,\n",
       "           -0.17929816],\n",
       "          [ 0.06048613,  0.02592263, -0.05400547, -0.138254  ,\n",
       "           -0.13393356],\n",
       "          [ 0.07128722, -0.07344744, -0.07560766, -0.09720985,\n",
       "           -0.07992809],\n",
       "          [ 0.07776788, -0.0345635 , -0.14689487, -0.13609378,\n",
       "           -0.06696678]]],\n",
       " \n",
       " \n",
       "        [[[ 0.12529269, -0.0453646 ,  0.12745291,  0.08208831,\n",
       "           -0.04752481],\n",
       "          [ 0.05184525,  0.04752481,  0.08856897,  0.03024306,\n",
       "           -0.06264634],\n",
       "          [-0.01728175,  0.        ,  0.06480657,  0.09288941,\n",
       "            0.02160219],\n",
       "          [ 0.04320437, -0.00648066, -0.03888394,  0.07560766,\n",
       "           -0.0453646 ],\n",
       "          [ 0.01296131, -0.10801094, -0.01080109, -0.0453646 ,\n",
       "            0.01080109]],\n",
       " \n",
       "         [[-0.1944197 ,  0.09504963,  0.00216022, -0.01944197,\n",
       "            0.07560766],\n",
       "          [-0.12097225, -0.07560766, -0.15337554, -0.07560766,\n",
       "            0.        ],\n",
       "          [-0.11233138, -0.138254  , -0.08856897, -0.16417663,\n",
       "           -0.04104416],\n",
       "          [-0.09720985, -0.08424854,  0.03888394, -0.03672372,\n",
       "           -0.03024306],\n",
       "          [ 0.1144916 ,  0.10369051,  0.09504963,  0.0453646 ,\n",
       "           -0.00432044]],\n",
       " \n",
       "         [[ 0.01728175,  0.0453646 ,  0.08424854, -0.06480657,\n",
       "           -0.01080109],\n",
       "          [ 0.04752481,  0.09288941,  0.02808284, -0.01944197,\n",
       "            0.01080109],\n",
       "          [ 0.08424854,  0.02808284,  0.03240328, -0.03024306,\n",
       "            0.09072919],\n",
       "          [ 0.04752481,  0.03024306, -0.09072919, -0.07992809,\n",
       "            0.06480657],\n",
       "          [-0.14257444,  0.02160219, -0.04104416,  0.0345635 ,\n",
       "            0.05184525]],\n",
       " \n",
       "         [[ 0.07776788, -0.01512153, -0.18577881, -0.1965799 ,\n",
       "           -0.10153028],\n",
       "          [-0.01512153,  0.09288941,  0.07992809, -0.07776788,\n",
       "           -0.11665181],\n",
       "          [ 0.00864088, -0.02808284, -0.00864088,  0.16417663,\n",
       "            0.12745291],\n",
       "          [ 0.00648066,  0.08640875,  0.10153028,  0.03888394,\n",
       "            0.07992809],\n",
       "          [ 0.06480657, -0.02592263, -0.11665181, -0.05400547,\n",
       "            0.03888394]],\n",
       " \n",
       "         [[-0.01512153,  0.09937006,  0.01080109,  0.00216022,\n",
       "            0.02808284],\n",
       "          [-0.10153028, -0.01512153, -0.02376241,  0.03672372,\n",
       "            0.05184525],\n",
       "          [ 0.01728175, -0.02160219, -0.03240328, -0.01080109,\n",
       "            0.05832591],\n",
       "          [ 0.04320437,  0.0345635 , -0.06696678, -0.07128722,\n",
       "           -0.03888394],\n",
       "          [ 0.0345635 , -0.04320437,  0.02160219, -0.09072919,\n",
       "            0.00864088]],\n",
       " \n",
       "         [[-0.09937006, -0.06480657,  0.10585072,  0.09504963,\n",
       "            0.02808284],\n",
       "          [ 0.12097225,  0.13393356,  0.12313247,  0.12961313,\n",
       "            0.12961313],\n",
       "          [ 0.06048613,  0.05184525,  0.00864088, -0.03024306,\n",
       "            0.07344744],\n",
       "          [-0.138254  , -0.15337554, -0.11017115, -0.03888394,\n",
       "           -0.12313247],\n",
       "          [-0.14257444, -0.04968503,  0.04752481, -0.04320437,\n",
       "           -0.08640875]]]], dtype=float32),\n",
       " 'conv2_weight_scale': 0.0021602187771350145,\n",
       " 'conv2_scale': 0.04141329973936081,\n",
       " 'conv2_zero_point': 0,\n",
       " 'fc1_weight': array([[ 26, -18,   0, ...,  10,  37,  -6],\n",
       "        [ -7, -34, -16, ...,  18, -15,   6],\n",
       "        [ -5, -17,  16, ...,  12,   5,  15],\n",
       "        ...,\n",
       "        [-33,  -5,  -8, ...,  29,   1, -17],\n",
       "        [  6, -19,  -7, ...,   0, -22, -32],\n",
       "        [ -9,   5,   5, ...,  -2,   1,  23]], dtype=int8),\n",
       " 'fc1_weight_raw': array([[ 0.05309297, -0.03675667,  0.        , ...,  0.02042037,\n",
       "          0.07555538, -0.01225222],\n",
       "        [-0.01429426, -0.06942927, -0.0326726 , ...,  0.03675667,\n",
       "         -0.03063056,  0.01225222],\n",
       "        [-0.01021019, -0.03471464,  0.0326726 , ...,  0.02450445,\n",
       "          0.01021019,  0.03063056],\n",
       "        ...,\n",
       "        [-0.06738724, -0.01021019, -0.0163363 , ...,  0.05921908,\n",
       "          0.00204204, -0.03471464],\n",
       "        [ 0.01225222, -0.03879871, -0.01429426, ...,  0.        ,\n",
       "         -0.04492483, -0.0653452 ],\n",
       "        [-0.01837834,  0.01021019,  0.01021019, ..., -0.00408407,\n",
       "          0.00204204,  0.04696686]], dtype=float32),\n",
       " 'fc1_weight_scale': 0.002042037434875965,\n",
       " 'fc1_scale': 0.1510031670331955,\n",
       " 'fc1_zero_point': 136,\n",
       " 'fc2_weight': array([[ 13, -51,  -3, ...,  -5,  35,  52],\n",
       "        [  0,  36,  31, ..., -35,   3,  29],\n",
       "        [ 30, -25, -45, ..., -22,   4, -21],\n",
       "        ...,\n",
       "        [ 19,  23,   5, ...,  12, -10,   8],\n",
       "        [ 22, -35, -17, ..., -22,  -2,   8],\n",
       "        [-41,  -9,  12, ...,  -4,  -1, -18]], dtype=int8),\n",
       " 'fc2_weight_raw': array([[ 0.0285341 , -0.11194148, -0.00658479, ..., -0.01097465,\n",
       "          0.07682259,  0.11413641],\n",
       "        [ 0.        ,  0.07901752,  0.06804286, ..., -0.07682259,\n",
       "          0.00658479,  0.063653  ],\n",
       "        [ 0.06584793, -0.05487328, -0.0987719 , ..., -0.04828848,\n",
       "          0.00877972, -0.04609355],\n",
       "        ...,\n",
       "        [ 0.04170369,  0.05048341,  0.01097465, ...,  0.02633917,\n",
       "         -0.02194931,  0.01755945],\n",
       "        [ 0.04828848, -0.07682259, -0.03731383, ..., -0.04828848,\n",
       "         -0.00438986,  0.01755945],\n",
       "        [-0.08999217, -0.01975438,  0.02633917, ..., -0.00877972,\n",
       "         -0.00219493, -0.03950876]], dtype=float32),\n",
       " 'fc2_weight_scale': 0.0021949310321360826,\n",
       " 'fc2_scale': 0.12524110078811646,\n",
       " 'fc2_zero_point': 115,\n",
       " 'fc3_weight': array([[  46,   23,  -13,  -12,  -24,   41,    0,   -9,   15,  -48,   52,\n",
       "          -45,   64,   29,   28,   15,   57,   -8,  -51,  -51,    6,  -48,\n",
       "          -14,  -39,   46,   53,  -82,  -27,    6,  -50,   -8,  -45,   -8,\n",
       "          -37,  -18,    4,   13,  -36,  -25,  -49,   31,  -23,   -4,  -52,\n",
       "           27,   40,   -7,   32,  -62,   41,  -12,  -14,  -18,  -29,  -85,\n",
       "          -11,    7,  -86,  -14,   39,  -21,   -8,  -89,  -51,   11,   50,\n",
       "          -10,  -32,  -40,  -47,   16,   -6,  -19,  -47,   68,   26,    0,\n",
       "           66,   15,  -38,    4,  -33,    9,  -14],\n",
       "        [  68,   36,  -17,   -1,   38,   22,  -11,   10,   13,   44,   32,\n",
       "          -18,   15,  -88,   -4,   -6,  -63,   40,  -21,  -24,  -84,   14,\n",
       "           18,   -1,  -33,   -4,   46,    5,   -9,   24,  -14,  -20,   26,\n",
       "          -15,   48,  -70,   45,   32,   25,   52,  -58,   -7,   27,    3,\n",
       "          -43,   54,  -68,   46,   36,  -72,   62,   68, -101,  -43,  -76,\n",
       "          -78,  -13,  -32,  -36,  -33,    2,   66,  -39,  -36,  -42,   28,\n",
       "            4,   66,    0,   75,  -36,   -7,   36,  -76,   50,  -27,  -40,\n",
       "           26,  -18,   26,   38,   36,  -79,  -69],\n",
       "        [  21,    0,   12,   71,   57,   51,  -52,  -47,   -5,  -49,  -50,\n",
       "          -15,  -19,  -46,   51,  -43,   39,  -15,  -69,   32,   -6,  -50,\n",
       "           19,   -7,  -39,   65,   12,  -46,    3,   21,   39,   -2,   55,\n",
       "           32,  -83,   43,  -86,   15,    7,  -62,   29,  -43,  -28,  -12,\n",
       "          -37,  -32,  -44,    1,  -20,   33,   42,  -29,  -54,   31,  -58,\n",
       "          -16,   42,  -85,   29,   20,  -14,    7,  -68,   49,  -46,  -19,\n",
       "           39,    7,  -58,   26,  -43,  -32,   -1,   30,   -6,   44,   20,\n",
       "          -53,   26,  -34,   22,    0,  -42,   61],\n",
       "        [   5,  -69,   35,  -31,   73,  -20,   17,  -32,  -49,  -36,  -29,\n",
       "           40,  -66,  -50,  -28,   42,   50,   -6,  -61,   28,  -56,   34,\n",
       "          -48,  -12,   61,  -50,   20,  -25,  -71,   50,  -24,   13,  -84,\n",
       "           10,   57,   12,  -51,  -49,  -18,  -38,   25,   -2,  -12,    9,\n",
       "           54,   69,  -13,   41,   29,   61,    9,  -62,   32,  -10,  -43,\n",
       "           29,   -8,   39,  -51,   16,   -3,    3,   53,  -35,   -6,  -15,\n",
       "          -34,   50,   -1, -109,  -42,   34,  -41,   28,   49,   -9,   40,\n",
       "          -24,  -69,   43,   50,   37,   65,  -68],\n",
       "        [  -3,   42,   36,    8,    6,   11,  -17,   27,  -75,    7,   -2,\n",
       "          -20,   41,   43,  -70,   20,  -43,  -12,   31,   53,  -59,   47,\n",
       "          -11,   44,  -78,  -78,   29,   -6,   63,    4,    2,   -9,   22,\n",
       "           14,  -53,  -58,   17,  -59,   -6,  -47,  -42,  -13,   23,    5,\n",
       "           43,  -71,   16,  -24,    5,   26,   42,   62,   -1,  -26,   21,\n",
       "           28,  -50,  -26,  -31,  -10,  -37,    6,   63,  -10,   26,   -6,\n",
       "           55,   44,   -7,   43,   51,   43,  -15,    5,  -81,  -44,   62,\n",
       "           38,   16,  -71,  -27,  -43,   33,  -46],\n",
       "        [ -38,   32,  -39,   -3,  -37,  -41,   63,   36,  -21,   -1,   69,\n",
       "           -2,  -57,    7,   54,   26,   11,  -84,  -69,   26,    2,  -16,\n",
       "           36,   -1,  -39,   25,   63,  -11,  -56,  -48,    8,  -11,  -17,\n",
       "          -23,   31,  -10,  -72,   12,  -33,   73,  -25,  -35,  -85,   39,\n",
       "           33,   24,   66,  -32,   19,  -16,    4,  -81,   10,   35,    5,\n",
       "          -67,  -63,   29,    8,   14,  -36,   -4,   28,  -13,   14,   40,\n",
       "          -30,  -83,   -6,   51,   61,  -10,  -28,  -50,  -25,  -41,   35,\n",
       "           55,   -8,  -15,   42,   17,   -2,   52],\n",
       "        [ -89,   52,  -44,  -32,   30,   46,  -32,   13,   -7,  -39,   47,\n",
       "           28,    0,  -33,   32,  -42,  -69,  -19,   19,   -1,   35,  -69,\n",
       "           28,  -31,   53,   59,  -60,  -39,   63,  -31,    2,  -33,  -22,\n",
       "           36,  -35,  -20,  -45,  -60,  -10,   54,   37,  -16,   30,   19,\n",
       "          -12,   30,   37,  -22,  -69,  -14,  -87,  -45,  -31,   51,   43,\n",
       "          -91,  -77,  -46,   25,   59,  -98,  -56,  -65,  -21,   32,   59,\n",
       "            5,  -23,   28,   40,   -2,   51,  -23,   -8,  -45,  -39,   54,\n",
       "           27,  -87,   10,  -69,  -89,   53,   -1],\n",
       "        [ -50,   78,  -45,  -21,  -19,  -84,   22,   10,   84,   31,   64,\n",
       "          -32,  -34,  -52,  -56,  -31,    8,   69,    2,  -57,  -43,   54,\n",
       "          -38,   -1,   25,  -38,   10,  -35,  -26,    1,   40,   16,   57,\n",
       "           38,  -84,  -35,   50,  -76,  -22,  -61,   50,  -21,  -33,   19,\n",
       "           37,  -21,  -47,   26,    1,  -51,   50,  -28,   26,  -11,   32,\n",
       "          -30,   -3,   35,  -50,   11,   62,   78,  -11,  -14,   36,   35,\n",
       "           21,    3,  -82,  -61,  -62,    4,  -56,   65,  -28,   -2,  -14,\n",
       "           60,   21,   30,   36,   72,  -14,   13],\n",
       "        [  19,   18,   -5,  -12,   29,  -75,   53,   20,  -21,  -51, -128,\n",
       "           48,  -13,  -65,   21,    2,   26,   56,  -11,  -47,   33,  -66,\n",
       "           18,   -6,  -55,    3,   21,   45,   40,   25,  -17,   16,  -65,\n",
       "          -46,   13,  -57,  -75,  -21,  -56,   14,  -38,   19,   16,    5,\n",
       "          -14,  -26,  -26,   41,  -49,   37,  -12,  -83,   34,   60,   39,\n",
       "           46,   73,   40,  -30,   -5,  -47,  -70,  -19,  -31,   55,   50,\n",
       "          -36,  -24,   71,  -67,   62,   44,   26,   58,  -12,   23,  -16,\n",
       "           -7,    7,  -44,   62,  -83,  -29,  -71],\n",
       "        [   0,   23,  -50,    3,   47,   -2,  -38,   10,  -36,  -26,  -32,\n",
       "            9,  -27,   39,   52,   -1,    6,  -10,  -29,  -44,   33,   26,\n",
       "          -19,   -4,  -72,   -9,   24,  -31,   38,  -81,   41,  -12,  -72,\n",
       "         -103,   -4,   26,   30,  -14,   21,   -2,  -16,  -44,  -38,   33,\n",
       "           32,  -50,   15,  -51,   67,   15,  -15,   93,   57,  -65,   26,\n",
       "            3,   -4,   49,   52,  -22,   17, -111,  -15,  -22,   14, -106,\n",
       "         -115,   52,   -7,  -14,   12,  -29,  -10,   41,   11,  -37,  -23,\n",
       "           57,    3,   40,    1,  -42,   90,   40]], dtype=int8),\n",
       " 'fc3_weight_raw': array([[ 0.09619249,  0.04809624, -0.02718483, -0.02509369, -0.05018738,\n",
       "          0.08573678,  0.        , -0.01882027,  0.03136712, -0.10037477,\n",
       "          0.10873933, -0.09410134,  0.13383302,  0.06064309,  0.05855194,\n",
       "          0.03136712,  0.11919504, -0.01672913, -0.10664819, -0.10664819,\n",
       "          0.01254685, -0.10037477, -0.02927597, -0.08155449,  0.09619249,\n",
       "          0.11083047, -0.17147356, -0.05646081,  0.01254685, -0.10455704,\n",
       "         -0.01672913, -0.09410134, -0.01672913, -0.07737222, -0.03764054,\n",
       "          0.00836456,  0.02718483, -0.07528108, -0.05227852, -0.10246591,\n",
       "          0.06482537, -0.04809624, -0.00836456, -0.10873933,  0.05646081,\n",
       "          0.08364564, -0.01463799,  0.06691651, -0.12965074,  0.08573678,\n",
       "         -0.02509369, -0.02927597, -0.03764054, -0.06064309, -0.17774698,\n",
       "         -0.02300255,  0.01463799, -0.17983812, -0.02927597,  0.08155449,\n",
       "         -0.04391396, -0.01672913, -0.18611154, -0.10664819,  0.02300255,\n",
       "          0.10455704, -0.02091141, -0.06691651, -0.08364564, -0.09828363,\n",
       "          0.03345826, -0.01254685, -0.03973168, -0.09828363,  0.14219758,\n",
       "          0.05436967,  0.        ,  0.1380153 ,  0.03136712, -0.07946336,\n",
       "          0.00836456, -0.06900765,  0.01882027, -0.02927597],\n",
       "        [ 0.14219758,  0.07528108, -0.03554939, -0.00209114,  0.07946336,\n",
       "          0.0460051 , -0.02300255,  0.02091141,  0.02718483,  0.0920102 ,\n",
       "          0.06691651, -0.03764054,  0.03136712, -0.1840204 , -0.00836456,\n",
       "         -0.01254685, -0.13174188,  0.08364564, -0.04391396, -0.05018738,\n",
       "         -0.17565584,  0.02927597,  0.03764054, -0.00209114, -0.06900765,\n",
       "         -0.00836456,  0.09619249,  0.01045571, -0.01882027,  0.05018738,\n",
       "         -0.02927597, -0.04182282,  0.05436967, -0.03136712,  0.10037477,\n",
       "         -0.14637987,  0.09410134,  0.06691651,  0.05227852,  0.10873933,\n",
       "         -0.12128618, -0.01463799,  0.05646081,  0.00627342, -0.08991906,\n",
       "          0.11292161, -0.14219758,  0.09619249,  0.07528108, -0.15056215,\n",
       "          0.12965074,  0.14219758, -0.21120523, -0.08991906, -0.15892671,\n",
       "         -0.16310899, -0.02718483, -0.06691651, -0.07528108, -0.06900765,\n",
       "          0.00418228,  0.1380153 , -0.08155449, -0.07528108, -0.08782792,\n",
       "          0.05855194,  0.00836456,  0.1380153 ,  0.        ,  0.15683557,\n",
       "         -0.07528108, -0.01463799,  0.07528108, -0.15892671,  0.10455704,\n",
       "         -0.05646081, -0.08364564,  0.05436967, -0.03764054,  0.05436967,\n",
       "          0.07946336,  0.07528108, -0.16520013, -0.14428872],\n",
       "        [ 0.04391396,  0.        ,  0.02509369,  0.14847101,  0.11919504,\n",
       "          0.10664819, -0.10873933, -0.09828363, -0.01045571, -0.10246591,\n",
       "         -0.10455704, -0.03136712, -0.03973168, -0.09619249,  0.10664819,\n",
       "         -0.08991906,  0.08155449, -0.03136712, -0.14428872,  0.06691651,\n",
       "         -0.01254685, -0.10455704,  0.03973168, -0.01463799, -0.08155449,\n",
       "          0.13592416,  0.02509369, -0.09619249,  0.00627342,  0.04391396,\n",
       "          0.08155449, -0.00418228,  0.11501275,  0.06691651, -0.1735647 ,\n",
       "          0.08991906, -0.17983812,  0.03136712,  0.01463799, -0.12965074,\n",
       "          0.06064309, -0.08991906, -0.05855194, -0.02509369, -0.07737222,\n",
       "         -0.06691651, -0.0920102 ,  0.00209114, -0.04182282,  0.06900765,\n",
       "          0.08782792, -0.06064309, -0.11292161,  0.06482537, -0.12128618,\n",
       "         -0.03345826,  0.08782792, -0.17774698,  0.06064309,  0.04182282,\n",
       "         -0.02927597,  0.01463799, -0.14219758,  0.10246591, -0.09619249,\n",
       "         -0.03973168,  0.08155449,  0.01463799, -0.12128618,  0.05436967,\n",
       "         -0.08991906, -0.06691651, -0.00209114,  0.06273423, -0.01254685,\n",
       "          0.0920102 ,  0.04182282, -0.11083047,  0.05436967, -0.07109879,\n",
       "          0.0460051 ,  0.        , -0.08782792,  0.1275596 ],\n",
       "        [ 0.01045571, -0.14428872,  0.07318994, -0.06482537,  0.15265329,\n",
       "         -0.04182282,  0.03554939, -0.06691651, -0.10246591, -0.07528108,\n",
       "         -0.06064309,  0.08364564, -0.1380153 , -0.10455704, -0.05855194,\n",
       "          0.08782792,  0.10455704, -0.01254685, -0.1275596 ,  0.05855194,\n",
       "         -0.11710389,  0.07109879, -0.10037477, -0.02509369,  0.1275596 ,\n",
       "         -0.10455704,  0.04182282, -0.05227852, -0.14847101,  0.10455704,\n",
       "         -0.05018738,  0.02718483, -0.17565584,  0.02091141,  0.11919504,\n",
       "          0.02509369, -0.10664819, -0.10246591, -0.03764054, -0.07946336,\n",
       "          0.05227852, -0.00418228, -0.02509369,  0.01882027,  0.11292161,\n",
       "          0.14428872, -0.02718483,  0.08573678,  0.06064309,  0.1275596 ,\n",
       "          0.01882027, -0.12965074,  0.06691651, -0.02091141, -0.08991906,\n",
       "          0.06064309, -0.01672913,  0.08155449, -0.10664819,  0.03345826,\n",
       "         -0.00627342,  0.00627342,  0.11083047, -0.07318994, -0.01254685,\n",
       "         -0.03136712, -0.07109879,  0.10455704, -0.00209114, -0.22793436,\n",
       "         -0.08782792,  0.07109879, -0.08573678,  0.05855194,  0.10246591,\n",
       "         -0.01882027,  0.08364564, -0.05018738, -0.14428872,  0.08991906,\n",
       "          0.10455704,  0.07737222,  0.13592416, -0.14219758],\n",
       "        [-0.00627342,  0.08782792,  0.07528108,  0.01672913,  0.01254685,\n",
       "          0.02300255, -0.03554939,  0.05646081, -0.15683557,  0.01463799,\n",
       "         -0.00418228, -0.04182282,  0.08573678,  0.08991906, -0.14637987,\n",
       "          0.04182282, -0.08991906, -0.02509369,  0.06482537,  0.11083047,\n",
       "         -0.12337732,  0.09828363, -0.02300255,  0.0920102 , -0.16310899,\n",
       "         -0.16310899,  0.06064309, -0.01254685,  0.13174188,  0.00836456,\n",
       "          0.00418228, -0.01882027,  0.0460051 ,  0.02927597, -0.11083047,\n",
       "         -0.12128618,  0.03554939, -0.12337732, -0.01254685, -0.09828363,\n",
       "         -0.08782792, -0.02718483,  0.04809624,  0.01045571,  0.08991906,\n",
       "         -0.14847101,  0.03345826, -0.05018738,  0.01045571,  0.05436967,\n",
       "          0.08782792,  0.12965074, -0.00209114, -0.05436967,  0.04391396,\n",
       "          0.05855194, -0.10455704, -0.05436967, -0.06482537, -0.02091141,\n",
       "         -0.07737222,  0.01254685,  0.13174188, -0.02091141,  0.05436967,\n",
       "         -0.01254685,  0.11501275,  0.0920102 , -0.01463799,  0.08991906,\n",
       "          0.10664819,  0.08991906, -0.03136712,  0.01045571, -0.16938242,\n",
       "         -0.0920102 ,  0.12965074,  0.07946336,  0.03345826, -0.14847101,\n",
       "         -0.05646081, -0.08991906,  0.06900765, -0.09619249],\n",
       "        [-0.07946336,  0.06691651, -0.08155449, -0.00627342, -0.07737222,\n",
       "         -0.08573678,  0.13174188,  0.07528108, -0.04391396, -0.00209114,\n",
       "          0.14428872, -0.00418228, -0.11919504,  0.01463799,  0.11292161,\n",
       "          0.05436967,  0.02300255, -0.17565584, -0.14428872,  0.05436967,\n",
       "          0.00418228, -0.03345826,  0.07528108, -0.00209114, -0.08155449,\n",
       "          0.05227852,  0.13174188, -0.02300255, -0.11710389, -0.10037477,\n",
       "          0.01672913, -0.02300255, -0.03554939, -0.04809624,  0.06482537,\n",
       "         -0.02091141, -0.15056215,  0.02509369, -0.06900765,  0.15265329,\n",
       "         -0.05227852, -0.07318994, -0.17774698,  0.08155449,  0.06900765,\n",
       "          0.05018738,  0.1380153 , -0.06691651,  0.03973168, -0.03345826,\n",
       "          0.00836456, -0.16938242,  0.02091141,  0.07318994,  0.01045571,\n",
       "         -0.14010644, -0.13174188,  0.06064309,  0.01672913,  0.02927597,\n",
       "         -0.07528108, -0.00836456,  0.05855194, -0.02718483,  0.02927597,\n",
       "          0.08364564, -0.06273423, -0.1735647 , -0.01254685,  0.10664819,\n",
       "          0.1275596 , -0.02091141, -0.05855194, -0.10455704, -0.05227852,\n",
       "         -0.08573678,  0.07318994,  0.11501275, -0.01672913, -0.03136712,\n",
       "          0.08782792,  0.03554939, -0.00418228,  0.10873933],\n",
       "        [-0.18611154,  0.10873933, -0.0920102 , -0.06691651,  0.06273423,\n",
       "          0.09619249, -0.06691651,  0.02718483, -0.01463799, -0.08155449,\n",
       "          0.09828363,  0.05855194,  0.        , -0.06900765,  0.06691651,\n",
       "         -0.08782792, -0.14428872, -0.03973168,  0.03973168, -0.00209114,\n",
       "          0.07318994, -0.14428872,  0.05855194, -0.06482537,  0.11083047,\n",
       "          0.12337732, -0.12546846, -0.08155449,  0.13174188, -0.06482537,\n",
       "          0.00418228, -0.06900765, -0.0460051 ,  0.07528108, -0.07318994,\n",
       "         -0.04182282, -0.09410134, -0.12546846, -0.02091141,  0.11292161,\n",
       "          0.07737222, -0.03345826,  0.06273423,  0.03973168, -0.02509369,\n",
       "          0.06273423,  0.07737222, -0.0460051 , -0.14428872, -0.02927597,\n",
       "         -0.18192926, -0.09410134, -0.06482537,  0.10664819,  0.08991906,\n",
       "         -0.19029383, -0.16101785, -0.09619249,  0.05227852,  0.12337732,\n",
       "         -0.20493181, -0.11710389, -0.13592416, -0.04391396,  0.06691651,\n",
       "          0.12337732,  0.01045571, -0.04809624,  0.05855194,  0.08364564,\n",
       "         -0.00418228,  0.10664819, -0.04809624, -0.01672913, -0.09410134,\n",
       "         -0.08155449,  0.11292161,  0.05646081, -0.18192926,  0.02091141,\n",
       "         -0.14428872, -0.18611154,  0.11083047, -0.00209114],\n",
       "        [-0.10455704,  0.16310899, -0.09410134, -0.04391396, -0.03973168,\n",
       "         -0.17565584,  0.0460051 ,  0.02091141,  0.17565584,  0.06482537,\n",
       "          0.13383302, -0.06691651, -0.07109879, -0.10873933, -0.11710389,\n",
       "         -0.06482537,  0.01672913,  0.14428872,  0.00418228, -0.11919504,\n",
       "         -0.08991906,  0.11292161, -0.07946336, -0.00209114,  0.05227852,\n",
       "         -0.07946336,  0.02091141, -0.07318994, -0.05436967,  0.00209114,\n",
       "          0.08364564,  0.03345826,  0.11919504,  0.07946336, -0.17565584,\n",
       "         -0.07318994,  0.10455704, -0.15892671, -0.0460051 , -0.1275596 ,\n",
       "          0.10455704, -0.04391396, -0.06900765,  0.03973168,  0.07737222,\n",
       "         -0.04391396, -0.09828363,  0.05436967,  0.00209114, -0.10664819,\n",
       "          0.10455704, -0.05855194,  0.05436967, -0.02300255,  0.06691651,\n",
       "         -0.06273423, -0.00627342,  0.07318994, -0.10455704,  0.02300255,\n",
       "          0.12965074,  0.16310899, -0.02300255, -0.02927597,  0.07528108,\n",
       "          0.07318994,  0.04391396,  0.00627342, -0.17147356, -0.1275596 ,\n",
       "         -0.12965074,  0.00836456, -0.11710389,  0.13592416, -0.05855194,\n",
       "         -0.00418228, -0.02927597,  0.12546846,  0.04391396,  0.06273423,\n",
       "          0.07528108,  0.15056215, -0.02927597,  0.02718483],\n",
       "        [ 0.03973168,  0.03764054, -0.01045571, -0.02509369,  0.06064309,\n",
       "         -0.15683557,  0.11083047,  0.04182282, -0.04391396, -0.10664819,\n",
       "         -0.26766604,  0.10037477, -0.02718483, -0.13592416,  0.04391396,\n",
       "          0.00418228,  0.05436967,  0.11710389, -0.02300255, -0.09828363,\n",
       "          0.06900765, -0.1380153 ,  0.03764054, -0.01254685, -0.11501275,\n",
       "          0.00627342,  0.04391396,  0.09410134,  0.08364564,  0.05227852,\n",
       "         -0.03554939,  0.03345826, -0.13592416, -0.09619249,  0.02718483,\n",
       "         -0.11919504, -0.15683557, -0.04391396, -0.11710389,  0.02927597,\n",
       "         -0.07946336,  0.03973168,  0.03345826,  0.01045571, -0.02927597,\n",
       "         -0.05436967, -0.05436967,  0.08573678, -0.10246591,  0.07737222,\n",
       "         -0.02509369, -0.1735647 ,  0.07109879,  0.12546846,  0.08155449,\n",
       "          0.09619249,  0.15265329,  0.08364564, -0.06273423, -0.01045571,\n",
       "         -0.09828363, -0.14637987, -0.03973168, -0.06482537,  0.11501275,\n",
       "          0.10455704, -0.07528108, -0.05018738,  0.14847101, -0.14010644,\n",
       "          0.12965074,  0.0920102 ,  0.05436967,  0.12128618, -0.02509369,\n",
       "          0.04809624, -0.03345826, -0.01463799,  0.01463799, -0.0920102 ,\n",
       "          0.12965074, -0.1735647 , -0.06064309, -0.14847101],\n",
       "        [ 0.        ,  0.04809624, -0.10455704,  0.00627342,  0.09828363,\n",
       "         -0.00418228, -0.07946336,  0.02091141, -0.07528108, -0.05436967,\n",
       "         -0.06691651,  0.01882027, -0.05646081,  0.08155449,  0.10873933,\n",
       "         -0.00209114,  0.01254685, -0.02091141, -0.06064309, -0.0920102 ,\n",
       "          0.06900765,  0.05436967, -0.03973168, -0.00836456, -0.15056215,\n",
       "         -0.01882027,  0.05018738, -0.06482537,  0.07946336, -0.16938242,\n",
       "          0.08573678, -0.02509369, -0.15056215, -0.21538752, -0.00836456,\n",
       "          0.05436967,  0.06273423, -0.02927597,  0.04391396, -0.00418228,\n",
       "         -0.03345826, -0.0920102 , -0.07946336,  0.06900765,  0.06691651,\n",
       "         -0.10455704,  0.03136712, -0.10664819,  0.14010644,  0.03136712,\n",
       "         -0.03136712,  0.19447611,  0.11919504, -0.13592416,  0.05436967,\n",
       "          0.00627342, -0.00836456,  0.10246591,  0.10873933, -0.0460051 ,\n",
       "          0.03554939, -0.23211664, -0.03136712, -0.0460051 ,  0.02927597,\n",
       "         -0.22166094, -0.24048121,  0.10873933, -0.01463799, -0.02927597,\n",
       "          0.02509369, -0.06064309, -0.02091141,  0.08573678,  0.02300255,\n",
       "         -0.07737222, -0.04809624,  0.11919504,  0.00627342,  0.08364564,\n",
       "          0.00209114, -0.08782792,  0.18820268,  0.08364564]],\n",
       "       dtype=float32),\n",
       " 'fc3_weight_scale': 0.002091140951961279,\n",
       " 'fc3_scale': 0.17506888508796692,\n",
       " 'fc3_zero_point': 142}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to fixed point format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant_scale\n",
      "quant_zero_point\n",
      "conv1_weight\n",
      "conv1_weight_raw\n",
      "conv1_weight_scale\n",
      "conv1_scale\n",
      "conv1_zero_point\n",
      "conv2_weight\n",
      "conv2_weight_raw\n",
      "conv2_weight_scale\n",
      "conv2_scale\n",
      "conv2_zero_point\n",
      "fc1_weight\n",
      "fc1_weight_raw\n",
      "fc1_weight_scale\n",
      "fc1_scale\n",
      "fc1_zero_point\n",
      "fc2_weight\n",
      "fc2_weight_raw\n",
      "fc2_weight_scale\n",
      "fc2_scale\n",
      "fc2_zero_point\n",
      "fc3_weight\n",
      "fc3_weight_raw\n",
      "fc3_weight_scale\n",
      "fc3_scale\n",
      "fc3_zero_point\n"
     ]
    }
   ],
   "source": [
    "for m in test_dict.keys():\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(test_dict[\"conv2_weight\"], np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_weight (6, 25)\n",
      "conv2_weight (16, 150)\n",
      "fc1_weight (120, 256)\n",
      "fc2_weight (84, 120)\n",
      "fc3_weight (10, 84)\n"
     ]
    }
   ],
   "source": [
    "for m, n in new_dict.items():\n",
    "    if isinstance(n, np.ndarray) and \"raw\" not in m:\n",
    "        if(n.ndim > 2):\n",
    "           n = n.reshape(n.shape[0], -1)\n",
    "        print(m, n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLenet(\n",
       "  (quant): Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027446914464235306, zero_point=0)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04141329973936081, zero_point=0)\n",
       "  (fc1): QuantizedLinear(in_features=256, out_features=120, scale=0.1510031670331955, zero_point=136, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=0.12524110078811646, zero_point=115, qscheme=torch.per_tensor_affine)\n",
       "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.17506888508796692, zero_point=142, qscheme=torch.per_tensor_affine)\n",
       "  (relu1): Identity()\n",
       "  (relu2): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_names = [\"layer1\", \"layer3\", \"layer5\", \"layer6\", \"layer7\"]\n",
    "base_folder = \"/scratch/joluseti/new_mif\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_q12_4(value):\n",
    "\n",
    "    # Convert the float to the fixed-point representation with 4 fractional bits\n",
    "    fixed_point_value = int(round(value * (1 << 4)))\n",
    "    #print(fixed_point_value)\n",
    "\n",
    "    # Format the value to 16 bits with leading zeros\n",
    "    return format(fixed_point_value & 0xFFFF, '016b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1111111111110000'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_to_q12_4(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(base_folder)\n",
    "base_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_weight processed\n",
      "conv2_weight processed\n",
      "fc1_weight processed\n",
      "fc2_weight processed\n",
      "fc3_weight processed\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for m, n in new_dict.items():\n",
    "    if isinstance(n, np.ndarray) and \"raw\" not in m:\n",
    "        if(n.ndim > 2):\n",
    "           n = n.reshape(n.shape[0], -1)\n",
    "        folder_path = base_path / folder_names[i]\n",
    "        folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for row_idx, row in enumerate(n):\n",
    "            file_path = folder_path / f\"{folder_names[i]}_weight{row_idx+1}.mif\"\n",
    "            #file_path.touch()\n",
    "            #file_path.unlink()\n",
    "            file_path.write_text(\"\\n\".join([str(float_to_q12_4(item)) for item in row]))\n",
    "            \n",
    "        print(f\"{m} processed\")\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder zipped as: /scratch/joluseti/new_mif.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the folder to zip and the output zip file\n",
    "folder_to_zip = \"example_folder\"\n",
    "output_zip = \"example_folder_compressed\"\n",
    "\n",
    "# Create a zip archive\n",
    "shutil.make_archive(base_folder, 'zip', base_folder)\n",
    "\n",
    "print(f\"Folder zipped as: {base_folder}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLenet(\n",
       "  (quant): QuantStub()\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=False)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=False)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=False)\n",
       "  (relu1): ReLU()\n",
       "  (relu2): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantLenet(\n",
       "  (quant): Quantize(scale=tensor([0.0127]), zero_point=tensor([33]), dtype=torch.quint8)\n",
       "  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(5, 5), stride=(1, 1), scale=0.027164967730641365, zero_point=0)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(5, 5), stride=(1, 1), scale=0.04062113165855408, zero_point=0)\n",
       "  (fc1): QuantizedLinear(in_features=256, out_features=120, scale=0.1486314982175827, zero_point=135, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): QuantizedLinear(in_features=120, out_features=84, scale=0.12291006743907928, zero_point=114, qscheme=torch.per_tensor_affine)\n",
       "  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.17384813725948334, zero_point=142, qscheme=torch.per_tensor_affine)\n",
       "  (relu1): Identity()\n",
       "  (relu2): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_weights = float_model.fc3.weight.detach().cpu().numpy()\n",
    "#quant_weights = quant_model.fc1.weight().int_repr().detach().cpu().numpy()\n",
    "quant_weights = quant_model.fc3.weight().dequantize().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Flatten the weights to make the histogram easier to plot\n",
    "float_weights_flat = float_weights.flatten()\n",
    "quant_weights_flat = quant_weights.flatten()\n",
    "\n",
    "# Plot the histograms for comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot the histogram for floating-point weights\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(float_weights_flat, density=False, bins=500, color='blue', alpha=0.7, label='Floating-Point Weights')\n",
    "plt.title('Histogram of Floating-Point Weights')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot the histogram for quantized weights\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(quant_weights_flat, density=False, bins=500, color='red', alpha=0.7, label='Quantized Weights')\n",
    "plt.title('Histogram of Quantized Weights')\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "651",
   "language": "python",
   "name": "651"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
